{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dc0f141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA TITAN Xp\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# GPU 사용 가능 여부 확인 (True가 나와야 함)\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# 현재 잡혀있는 GPU 장치 이름 출력\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3a765ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving TSP (N=14) with PyTorch/CUDA\n",
      "[Iter 0] Cost: 207.06 (New Best!)\n",
      "[Iter 1] Cost: 207.06\n",
      "[Iter 2] Cost: 207.06\n",
      "[Iter 3] Cost: 207.06\n",
      "[Iter 4] Cost: 207.06\n",
      "[Iter 5] Cost: 207.06\n",
      "[Iter 6] Cost: 207.06\n",
      "[Iter 7] Cost: 207.06\n",
      "[Iter 8] Cost: 207.06\n",
      "[Iter 9] Cost: 207.06\n",
      "[Iter 10] Cost: 207.06\n",
      "[Iter 11] Cost: 207.06\n",
      "[Iter 12] Cost: 207.06\n",
      "[Iter 13] Cost: 207.06\n",
      "[Iter 14] Cost: 207.06\n",
      "[Iter 15] Cost: 207.06\n",
      "[Iter 16] Cost: 207.06\n",
      "[Iter 17] Cost: 207.06\n",
      "[Iter 18] Cost: 207.06\n",
      "[Iter 19] Cost: 207.06\n",
      "Final Path: [14, 12, 7, 9, 13, 10, 1, 0, 6, 3, 5, 2, 4, 8, 11, 14]\n",
      "Final Cost: 207.05797\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "\n",
    "class TSPSolverSOVATorch:\n",
    "    def __init__(self, dist_matrix, bp_iterations=20, damping=0.7, verbose=True, device='cuda'):\n",
    "        # Device 설정\n",
    "        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # 데이터 초기화 및 Tensor 변환\n",
    "        self.dist_matrix = torch.tensor(dist_matrix, dtype=torch.float32, device=self.device)\n",
    "        self.num_nodes = self.dist_matrix.shape[0]\n",
    "        self.N = self.num_nodes - 1\n",
    "        self.depot = self.N\n",
    "        self.bp_iterations = bp_iterations\n",
    "        self.damping = damping\n",
    "        self.verbose = verbose\n",
    "        self.INF = 1e8\n",
    "        self.accumulated_bias = torch.zeros((self.N, self.N), device=self.device)\n",
    "        \n",
    "        # 비트마스크 전체 크기 (2^N)\n",
    "        self.num_states = 1 << self.N\n",
    "        self.FULL_MASK = self.num_states - 1\n",
    "        \n",
    "        # S Matrix 계산 (Max - Dist)\n",
    "        max_dist = torch.max(self.dist_matrix)\n",
    "        self.S = max_dist - self.dist_matrix\n",
    "        self.S.fill_diagonal_(-self.INF)\n",
    "        \n",
    "        # Messages initialization (N x N)\n",
    "        self.tilde_rho = torch.zeros((self.N, self.N), device=self.device)\n",
    "        self.tilde_eta = torch.zeros((self.N, self.N), device=self.device)\n",
    "        self.tilde_phi = torch.zeros((self.N, self.N), device=self.device)\n",
    "        \n",
    "        # Precompute Masks by Population Count (CPU에서 계산 후 GPU 인덱스로 변환)\n",
    "        # DP 수행 시 같은 '방문 도시 수(popcount)'를 가진 상태끼리 묶어서 병렬 처리\n",
    "        self.masks_by_popcount = [[] for _ in range(self.N + 1)]\n",
    "        for mask in range(self.num_states):\n",
    "            cnt = bin(mask).count('1')\n",
    "            if cnt <= self.N:\n",
    "                self.masks_by_popcount[cnt].append(mask)\n",
    "        \n",
    "        # GPU Tensor로 변환 (인덱싱용)\n",
    "        self.masks_by_popcount_t = [\n",
    "            torch.tensor(m, dtype=torch.long, device=self.device) \n",
    "            for m in self.masks_by_popcount\n",
    "        ]\n",
    "\n",
    "    def log(self, msg):\n",
    "        if self.verbose:\n",
    "            print(msg)\n",
    "\n",
    "    def _calc_lambda_sum_bias(self):\n",
    "        \"\"\"\n",
    "        [수정됨] Positive Feedback for Score Maximization\n",
    "        Score(이득)을 최대화하는 문제이므로, \n",
    "        방문 확률(rho)이 높은 곳에 가산점(+)을 줘야 경로가 유지됨.\n",
    "        \"\"\"\n",
    "        N = self.N\n",
    "        \n",
    "        # 1. Row Sum\n",
    "        sum_rho_t = torch.sum(self.tilde_rho, dim=1, keepdim=True)\n",
    "        \n",
    "        # 2. Gradient 계산 (부호 수정됨!)\n",
    "        # - 기존: -rho (Negative Feedback) -> 1등 경로 파괴\n",
    "        # - 수정: +rho (Positive Feedback) -> 1등 경로 강화\n",
    "        # - 뒤쪽 항: Row Normalization을 위해 빼줌 (Centering)\n",
    "        \n",
    "        current_grad = self.tilde_rho - ((N - 1) / N) * sum_rho_t\n",
    "        \n",
    "        # 3. Bias 누적\n",
    "        step_size = 1  # 학습률\n",
    "        self.accumulated_bias += step_size * current_grad\n",
    "        \n",
    "        return self.accumulated_bias\n",
    "\n",
    "    def _run_trellis_gpu(self):\n",
    "        \"\"\"\n",
    "        GPU Optimized Trellis (Forward/Backward)\n",
    "        Dictionary 대신 Dense Tensor [2^N, N] 사용\n",
    "        \"\"\"\n",
    "        N, S, depot = self.N, self.S, self.depot\n",
    "        bias = self._calc_lambda_sum_bias() # (N, N)\n",
    "        \n",
    "        # --- [1] Forward (Alpha) ---\n",
    "        # alpha[mask, last_node]\n",
    "        alpha = torch.full((self.num_states, N), -self.INF, device=self.device)\n",
    "        \n",
    "        # 초기 상태: 마스크 0은 없으므로, 첫 방문 노드들에 대해 설정\n",
    "        # 0번 step(depot에서 출발)은 미리 처리\n",
    "        # depot -> node i\n",
    "        # mask: 1 << i, score: S[depot, i] + bias[0, i] (bias는 시간 t=0)\n",
    "        \n",
    "        # t=0 초기화\n",
    "        start_bias = bias[0] # (N,)\n",
    "        start_scores = S[depot, :N] + start_bias # (N,)\n",
    "        \n",
    "        # 각 i에 대해 mask = 1<<i 위치에 값 할당\n",
    "        initial_masks = (1 << torch.arange(N, device=self.device))\n",
    "        alpha[initial_masks, torch.arange(N, device=self.device)] = start_scores # deprecated indexing fix applied conceptually\n",
    "\n",
    "        # DP Loop (t = 1 to N-1)\n",
    "        for t in range(1, N):\n",
    "            current_bias = bias[t] # (N,)\n",
    "            prev_masks = self.masks_by_popcount_t[t]     # 현재 방문 수 t인 마스크들\n",
    "            next_masks_indices = self.masks_by_popcount_t[t+1] # 방문 수 t+1인 마스크들 (검증용 혹은 인덱싱용)\n",
    "            \n",
    "            # 현재 유효한 점수 가져오기: (Num_Masks, N)\n",
    "            curr_scores = alpha[prev_masks, :] \n",
    "            \n",
    "            # Transition: (Num_Masks, N_prev) -> (Num_Masks, N_prev, N_next)\n",
    "            # prev에서 next로 갈 때의 비용 추가\n",
    "            # S: (N, N), current_bias: (N,)\n",
    "            # cost[prev, next] = S[prev, next] + current_bias[next]\n",
    "            transition_cost = S[:N, :N] + current_bias.view(1, N) # (N, N)\n",
    "            \n",
    "            # BroadCasting:\n",
    "            # curr_scores: (M, N, 1)\n",
    "            # transition:  (1, N, N)\n",
    "            # candidates:  (M, N, N) -> (M, N_prev, N_next)\n",
    "            candidates = curr_scores.unsqueeze(2) + transition_cost.unsqueeze(0)\n",
    "            \n",
    "            # 유효하지 않은 경로(이미 방문한 노드) 필터링은 bit logic으로 해야 함\n",
    "            # M개의 마스크 각각에 대해 next_node 비트가 0이어야 함\n",
    "            # Tensor 연산으로 마스크 계산: (M, 1) | (1 << (N)) \n",
    "            # -> (M, N_next)\n",
    "            \n",
    "            mask_col = prev_masks.view(-1, 1) # (M, 1)\n",
    "            shifts = torch.arange(N, device=self.device).view(1, -1) # (1, N)\n",
    "            next_bit_check = (mask_col & (1 << shifts)) == 0 # (M, N) : 방문 안했으면 True\n",
    "            \n",
    "            # 방문한 곳은 -INF 처리\n",
    "            # (M, N, N) 에서 next node(3번째 차원) 기준으로 마스킹\n",
    "            valid_mask = next_bit_check.unsqueeze(1).expand(-1, N, -1) # (M, N_prev, N_next)\n",
    "            candidates = torch.where(valid_mask, candidates, torch.tensor(-self.INF, device=self.device))\n",
    "            \n",
    "            # Flatten candidates to scatter\n",
    "            # 목적지 Mask 계산: old_mask | (1 << next_node)\n",
    "            # (M, N_next) 행렬 생성\n",
    "            new_masks_calc = mask_col | (1 << shifts) # (M, N_next)\n",
    "            \n",
    "            # Scatter를 위해 데이터 평탄화\n",
    "            # 우리는 각 (new_mask, next_node)에 대해 Max 값을 구해야 함\n",
    "            \n",
    "            # Source values: candidates (M, N, N)\n",
    "            src_vals = candidates.reshape(-1)\n",
    "            \n",
    "            # Destination indices construction\n",
    "            # alpha는 (2^N, N) 형태. flat index = mask * N + node\n",
    "            \n",
    "            # 각 후보의 new_mask: (M, 1, N) -> expand -> (M, N, N)\n",
    "            new_masks_expanded = new_masks_calc.unsqueeze(1).expand(-1, N, -1).reshape(-1)\n",
    "            next_nodes_expanded = shifts.expand(len(prev_masks), N, N).reshape(-1) # 틀림, logic 수정 필요\n",
    "            \n",
    "            # 정확한 인덱스 생성:\n",
    "            # Outer loop: Masks(M), Middle: Prev(N), Inner: Next(N)\n",
    "            # new_mask는 Prev와 무관하게 (Mask | Next)임.\n",
    "            m_idx = new_masks_calc.unsqueeze(1).expand(-1, N, -1).reshape(-1) # (M*N*N)\n",
    "            n_idx = torch.arange(N, device=self.device).view(1, 1, N).expand(len(prev_masks), N, -1).reshape(-1)\n",
    "            \n",
    "            flat_indices = m_idx * N + n_idx\n",
    "            \n",
    "            # scatter_reduce_ (PyTorch 1.12+) 사용\n",
    "            # alpha.view(-1) 에 src_vals를 max로 업데이트\n",
    "            alpha.view(-1).scatter_reduce_(0, flat_indices, src_vals, reduce='amax', include_self=True)\n",
    "\n",
    "        # --- [2] Backward (Beta) ---\n",
    "        # Beta 역시 Dense Tensor [2^N, N]\n",
    "        beta = torch.full((self.num_states, N), -self.INF, device=self.device)\n",
    "        \n",
    "        # t=N (마지막) 초기화: Depot으로 돌아가는 비용\n",
    "        # beta[FULL_MASK, i] = S[i, depot]\n",
    "        beta[self.FULL_MASK, :] = S[:N, depot]\n",
    "        \n",
    "        # Backward Loop (t = N-1 down to 0)\n",
    "        for t in range(N - 1, -1, -1):\n",
    "            curr_bias = bias[t] # (N,)\n",
    "            \n",
    "            # t+1 시점의 마스크들\n",
    "            next_masks = self.masks_by_popcount_t[t+1]\n",
    "            if len(next_masks) == 0: continue\n",
    "            \n",
    "            # (M_next, N)\n",
    "            next_beta_vals = beta[next_masks, :] \n",
    "            \n",
    "            # Xi 계산: beta + bias[node]\n",
    "            # (M_next, N)\n",
    "            xi_val = next_beta_vals + curr_bias.view(1, N)\n",
    "            \n",
    "            # 이번에는 '이전 노드(prev)'를 찾아야 함\n",
    "            # next_mask에서 next_node 비트를 끈 것이 prev_mask\n",
    "            # prev_node -> next_node\n",
    "            \n",
    "            # (M_next, N_next) -> next_node가 i일 때의 xi값\n",
    "            # 우리는 모든 가능한 prev_node j에 대해:\n",
    "            # val = S[j, i] + xi_val[mask, i]\n",
    "            # update beta[mask ^ (1<<i), j] with max(val)\n",
    "            \n",
    "            # S: (N, N) (row=prev, col=next)\n",
    "            # xi_val: (M, N) (row=mask, col=next)\n",
    "            \n",
    "            # Broadcasting 합: (N, N) + (M, 1, N) -> (M, prev, next) ?\n",
    "            # (M, 1, N_next) + (1, N_prev, N_next) -> (M, N_prev, N_next)\n",
    "            \n",
    "            candidates = xi_val.unsqueeze(1) + S[:N, :N].unsqueeze(0)\n",
    "            \n",
    "            # Valid Check: prev_node가 next_mask에 포함되어 있어야 함\n",
    "            # (역추적이니까, next_mask에 있는 비트들 중 하나가 prev_node가 됨... 아님)\n",
    "            # 정확히는: next_mask에서 i를 뺀 것이 t시점의 mask.\n",
    "            # 그리고 그 t시점 mask에 j가 포함되어 있어야 j -> i가 가능?\n",
    "            # 아니, Backward는 \"미래에 i를 방문했고 상태가 next_mask였다면, 현재 j에서의 가치\"\n",
    "            # 즉, next_mask 상태는 j를 이미 방문한 상태여야 함.\n",
    "            \n",
    "            mask_col = next_masks.view(-1, 1) # (M, 1)\n",
    "            shifts = torch.arange(N, device=self.device).view(1, -1) # (1, N)\n",
    "            \n",
    "            # next_node(i)가 mask에 포함되어 있어야 유효한 출발점\n",
    "            has_next_node = (mask_col & (1 << shifts)) != 0 # (M, N_next)\n",
    "            \n",
    "            # prev_mask 계산: mask ^ (1<<i)\n",
    "            prev_masks_calc = mask_col ^ (1 << shifts)\n",
    "            \n",
    "            # j (prev_node)가 prev_mask에 포함되어 있어야 함 (단, t=0일 땐 prev_mask가 0이므로 예외)\n",
    "            if t > 0:\n",
    "                 # (M, N_next, 1) & (1, 1, N_prev)\n",
    "                 prev_has_j = (prev_masks_calc.unsqueeze(2) & (1 << shifts).unsqueeze(0).unsqueeze(0)) != 0\n",
    "                 # (M, N_next, N_prev)\n",
    "            else:\n",
    "                 # t=0이면 prev_mask는 0이어야 하고, prev_node는 없음(Depot).\n",
    "                 # 하지만 코드 구조상 t=0까지 루프를 돌며 beta[0, :] 등을 채움?\n",
    "                 # 원본 코드는 t=0까지 돌고, cands=[depot] 처리함.\n",
    "                 # 여기선 NxN 구조만 다루므로 t=0 단계는 사실상 beta update 필요 없음 (depot 연결은 별도)\n",
    "                 pass\n",
    "            \n",
    "            # --- Backward Scatter Logic ---\n",
    "            # candidates: (M, N_prev, N_next) = Cost(j->i) + Beta(next_mask, i)\n",
    "            # Target: beta[prev_mask, j]\n",
    "            \n",
    "            # 유효성 마스킹\n",
    "            valid_mask = has_next_node.unsqueeze(1).expand(-1, N, -1) # i가 mask에 있어야함\n",
    "            if t > 0:\n",
    "                valid_mask = valid_mask & prev_has_j\n",
    "            \n",
    "            # t=0일때는 prev_mask가 0. (0, j)에 업데이트? \n",
    "            # 사실 t=0에서 beta값은 필요 없거나 depot 연결용. 여기선 생략 가능하지만 구조 유지.\n",
    "            \n",
    "            vals = torch.where(valid_mask, candidates, torch.tensor(-self.INF, device=self.device))\n",
    "            \n",
    "            # Scatter Indices\n",
    "            # Target Mask: prev_masks_calc (M, N_next) -> (M, N_prev, N_next) 확장\n",
    "            p_mask_idx = prev_masks_calc.unsqueeze(1).expand(-1, N, -1).reshape(-1)\n",
    "            p_node_idx = torch.arange(N, device=self.device).view(1, N, 1).expand(len(next_masks), -1, N).reshape(-1)\n",
    "            \n",
    "            flat_indices = p_mask_idx * N + p_node_idx\n",
    "            src_vals = vals.reshape(-1)\n",
    "            \n",
    "            beta.view(-1).scatter_reduce_(0, flat_indices, src_vals, reduce='amax', include_self=True)\n",
    "\n",
    "        # --- [3] Soft Output (Delta) Calculation ---\n",
    "        # Vectorized Max-In / Max-Out\n",
    "        tilde_delta = torch.zeros((N, N), device=self.device)\n",
    "        \n",
    "        # 모든 시간 t에 대해 병렬 처리는 메모리 부담, t별 Loop\n",
    "        for t in range(N):\n",
    "            # Alpha(t) + Beta(t) = Total Score Map\n",
    "            # shape: (Masks_t, N)\n",
    "            curr_masks = self.masks_by_popcount_t[t+1] # t+1 시점의 alpha, beta 사용 (원본 코드 참조)\n",
    "            if len(curr_masks) == 0: continue\n",
    "            \n",
    "            # alpha: (M, N), beta: (M, N)\n",
    "            a = alpha[curr_masks]\n",
    "            b = beta[curr_masks]\n",
    "            \n",
    "            # 유효한 값들만 더함 (-INF 방지)\n",
    "            valid = (a > -self.INF/2) & (b > -self.INF/2)\n",
    "            scores = torch.where(valid, a + b, torch.tensor(-self.INF, device=self.device))\n",
    "            \n",
    "            # City Max Scores: 해당 시간 t에 각 도시 i를 방문했을 때의 최대 점수\n",
    "            # Reduce over Masks dimension -> (N,)\n",
    "            city_max_scores = torch.max(scores, dim=0)[0] # (N,)\n",
    "            \n",
    "            # Find Best and Second Best for Max-Out\n",
    "            # topk (2 values)\n",
    "            top2_vals, top2_idxs = torch.topk(city_max_scores, 2)\n",
    "            global_max = top2_vals[0]\n",
    "            global_second = top2_vals[1]\n",
    "            best_idx = top2_idxs[0]\n",
    "            \n",
    "            # Rho bias\n",
    "            rho_t = self.tilde_rho[t]\n",
    "            lam_i_for_i = -(1.0/N) * rho_t\n",
    "            lam_i_for_j = ((N-1.0)/N) * rho_t\n",
    "            \n",
    "            max_in = city_max_scores - lam_i_for_i\n",
    "            \n",
    "            # Max Out Vectorized\n",
    "            # 기본적으로 global_max 사용, best_idx 위치만 global_second 사용\n",
    "            max_out_raw = torch.full((N,), global_max, device=self.device)\n",
    "            max_out_raw[best_idx] = global_second\n",
    "            \n",
    "            max_out = max_out_raw - lam_i_for_j\n",
    "            \n",
    "            # Diff calculation\n",
    "            diff = max_in - max_out\n",
    "            \n",
    "            # INF handling\n",
    "            diff = torch.where(max_in < -self.INF/2, torch.tensor(-self.INF, device=self.device), diff)\n",
    "            diff = torch.where(max_out < -self.INF/2, torch.tensor(self.INF, device=self.device), diff)\n",
    "            \n",
    "            tilde_delta[t] = diff\n",
    "            \n",
    "        return alpha, tilde_delta\n",
    "\n",
    "    def _run_bp_gpu(self, tilde_delta):\n",
    "        \"\"\"Matrix Operations Fully on GPU\"\"\"\n",
    "        N = self.N\n",
    "        \n",
    "        # 1. Omega 계산\n",
    "        t_omega = self.tilde_phi + tilde_delta\n",
    "        \n",
    "        # 2. Eta 계산 (Column-wise Max excluding self)\n",
    "        vals, idxs = torch.topk(t_omega, 2, dim=0) # (2, N)\n",
    "        max1 = vals[0]\n",
    "        max2 = vals[1]\n",
    "        argmax = idxs[0]\n",
    "        \n",
    "        rows = torch.arange(N, device=self.device).view(N, 1).expand(N, N)\n",
    "        is_max_pos = (rows == argmax)\n",
    "        \n",
    "        new_eta = torch.where(is_max_pos, max2, max1)\n",
    "        new_eta = -new_eta\n",
    "        \n",
    "        # 3. Gamma 계산\n",
    "        t_gamma = new_eta + tilde_delta\n",
    "        \n",
    "        # 4. Phi 계산 (Row-wise Max excluding self)\n",
    "        vals_r, idxs_r = torch.topk(t_gamma, 2, dim=1)\n",
    "        max1_r = vals_r[:, 0].unsqueeze(1)\n",
    "        max2_r = vals_r[:, 1].unsqueeze(1)\n",
    "        argmax_r = idxs_r[:, 0].unsqueeze(1)\n",
    "        \n",
    "        cols = torch.arange(N, device=self.device).view(1, N).expand(N, N)\n",
    "        is_max_pos_r = (cols == argmax_r)\n",
    "        \n",
    "        new_phi = torch.where(is_max_pos_r, max2_r, max1_r)\n",
    "        new_phi = -new_phi\n",
    "        \n",
    "        # Update with Damping\n",
    "        self.tilde_eta = self.damping * self.tilde_eta + (1 - self.damping) * new_eta\n",
    "        self.tilde_phi = self.damping * self.tilde_phi + (1 - self.damping) * new_phi\n",
    "        \n",
    "        # ---------------------------------------------------------\n",
    "        # [수정됨] Double Centering 적용\n",
    "        # ---------------------------------------------------------\n",
    "        # 1. Rho 계산\n",
    "        raw_rho = self.tilde_eta + self.tilde_phi\n",
    "        \n",
    "        # 2. Row Centering (각 시간 t에 대해 평균 0)\n",
    "        # 식 (4.2) 의 조건과 유사하게 Row sum constraints 보정\n",
    "        row_mean = torch.mean(raw_rho, dim=1, keepdim=True)\n",
    "        raw_rho -= row_mean\n",
    "        \n",
    "        # 3. Column Centering (각 도시 i에 대해 평균 0) - 이게 핵심!\n",
    "        # 특정 도시로 메시지가 쏠리는 것을 방지\n",
    "        col_mean = torch.mean(raw_rho, dim=0, keepdim=True)\n",
    "        raw_rho -= col_mean\n",
    "        \n",
    "        # 4. 최종 할당\n",
    "        self.tilde_rho = raw_rho\n",
    "\n",
    "    def _extract_path_gpu(self, alpha):\n",
    "            # CPU로 가져와서 순차적으로 역추적 (이 부분은 계산량이 적으므로 CPU가 편함)\n",
    "            # alpha_cpu = alpha.cpu().numpy() # (주석 처리됨)\n",
    "            \n",
    "            path = []\n",
    "            curr_mask = self.FULL_MASK\n",
    "            \n",
    "            # 1) 마지막 도시 선택\n",
    "            # alpha[FULL_MASK, i] + S[i, depot]\n",
    "            # S에서도 N개의 도시에 해당하는 부분만 가져오도록 슬라이싱 [:self.N] 필수\n",
    "            final_scores = alpha[self.FULL_MASK, :] + self.S[:self.N, self.depot]\n",
    "            best_last = torch.argmax(final_scores).item()\n",
    "            best_score = final_scores[best_last].item()\n",
    "            \n",
    "            if best_score < -self.INF/2:\n",
    "                return [], self.INF\n",
    "                \n",
    "            path = [self.depot, best_last]\n",
    "            curr_node = best_last\n",
    "            curr_mask = self.FULL_MASK ^ (1 << best_last)\n",
    "            \n",
    "            # 2) 역추적\n",
    "            for t in range(self.N - 1, 0, -1):\n",
    "                # alpha[t] 개념이 아니라, alpha[curr_mask, prev] + S[prev, curr_node]\n",
    "                \n",
    "                # [수정된 부분] self.S[:, curr_node] -> self.S[:self.N, curr_node]\n",
    "                # S 행렬은 (N+1 x N+1) 크기이므로, alpha(N)와 크기를 맞추기 위해 \n",
    "                # 앞쪽 N개의 행(도시들)만 슬라이싱해야 합니다.\n",
    "                scores = alpha[curr_mask, :] + self.S[:self.N, curr_node]\n",
    "                \n",
    "                best_prev = torch.argmax(scores).item()\n",
    "                val = scores[best_prev].item()\n",
    "                \n",
    "                if val < -self.INF/2:\n",
    "                    print(f\"Traceback failed at t={t}\")\n",
    "                    return [], self.INF\n",
    "                    \n",
    "                path.append(best_prev)\n",
    "                curr_node = best_prev\n",
    "                curr_mask ^= (1 << best_prev)\n",
    "                \n",
    "            path.append(self.depot)\n",
    "            path.reverse()\n",
    "            \n",
    "            # Cost Recalculation (CPU)\n",
    "            dist_cpu = self.dist_matrix.cpu().numpy()\n",
    "            cost = sum(dist_cpu[path[k], path[k+1]] for k in range(len(path)-1))\n",
    "            \n",
    "            return path, cost\n",
    "\n",
    "    def solve(self):\n",
    "        best_global_path = []\n",
    "        best_global_cost = self.INF\n",
    "        \n",
    "        print(f\"Solving TSP (N={self.N}) with PyTorch/CUDA\")\n",
    "        \n",
    "        for it in range(self.bp_iterations):\n",
    "            alpha, tilde_delta = self._run_trellis_gpu()\n",
    "            path, cost = self._extract_path_gpu(alpha)\n",
    "            \n",
    "            if cost < best_global_cost:\n",
    "                best_global_cost = cost\n",
    "                best_global_path = path\n",
    "                print(f\"[Iter {it}] Cost: {cost:.2f} (New Best!)\")\n",
    "            elif self.verbose:\n",
    "                print(f\"[Iter {it}] Cost: {cost:.2f}\")\n",
    "\n",
    "            self._run_bp_gpu(tilde_delta)\n",
    "            \n",
    "        return best_global_path, best_global_cost\n",
    "\n",
    "# --- 사용 예시 ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 임의의 거리 행렬 생성 (5x5)\n",
    "    # 실제 데이터는 파일에서 로드하거나 더 큰 N 사용\n",
    "    N_CITIES = 15\n",
    "    data = np.random.rand(N_CITIES, N_CITIES)*100\n",
    "    # TSP는 대칭이 아니어도 되지만, 보통 대각선 0\n",
    "    np.fill_diagonal(data, 0)\n",
    "    \n",
    "    solver = TSPSolverSOVATorch(data, bp_iterations=20, damping=0.5, verbose=True)\n",
    "    path, cost = solver.solve()\n",
    "    print(\"Final Path:\", path)\n",
    "    print(\"Final Cost:\", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cb29ed6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving TSP (N=19) - Original Definition (Lambda=Rho)\n",
      "[Iter 0] Cost: 142.63 (New Best!)\n",
      "[Iter 1] Cost: 142.63\n",
      "[Iter 2] Cost: 142.63\n",
      "[Iter 3] Cost: 142.63\n",
      "[Iter 4] Cost: 142.63\n",
      "[Iter 5] Cost: 142.63\n",
      "[Iter 6] Cost: 142.63\n",
      "[Iter 7] Cost: 142.63\n",
      "[Iter 8] Cost: 142.63\n",
      "[Iter 9] Cost: 142.63\n",
      "[Iter 10] Cost: 142.63\n",
      "[Iter 11] Cost: 142.63\n",
      "[Iter 12] Cost: 142.63\n",
      "[Iter 13] Cost: 142.63\n",
      "[Iter 14] Cost: 142.63\n",
      "[Iter 15] Cost: 142.63\n",
      "[Iter 16] Cost: 142.63\n",
      "[Iter 17] Cost: 142.63\n",
      "[Iter 18] Cost: 142.63\n",
      "[Iter 19] Cost: 142.63\n",
      "Final Path: [19, 7, 5, 9, 1, 12, 4, 18, 14, 10, 8, 11, 17, 13, 2, 16, 15, 0, 6, 3, 19]\n",
      "Final Cost: 142.62875\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class TSPSolverSOVATorch:\n",
    "    def __init__(self, dist_matrix, bp_iterations=20, damping=0.7, verbose=True, device='cuda'):\n",
    "        # Device 설정\n",
    "        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # 데이터 초기화 및 Tensor 변환\n",
    "        self.dist_matrix = torch.tensor(dist_matrix, dtype=torch.float32, device=self.device)\n",
    "        self.num_nodes = self.dist_matrix.shape[0]\n",
    "        self.N = self.num_nodes - 1\n",
    "        self.depot = self.N\n",
    "        self.bp_iterations = bp_iterations\n",
    "        self.damping = damping  # BP 메시지 업데이트 시의 관성(Momentum)\n",
    "        self.verbose = verbose\n",
    "        self.INF = 1e8\n",
    "        \n",
    "        # [변경] accumulated_bias 제거 (Gradient 방식 폐기)\n",
    "        # 본래 BP 정의대로 매 iter마다 메시지를 새로 계산하여 전달합니다.\n",
    "\n",
    "        # 비트마스크 전체 크기 (2^N)\n",
    "        self.num_states = 1 << self.N\n",
    "        self.FULL_MASK = self.num_states - 1\n",
    "        \n",
    "        # S Matrix 계산 (Max - Dist)\n",
    "        max_dist = torch.max(self.dist_matrix)\n",
    "        self.S = max_dist - self.dist_matrix\n",
    "        self.S.fill_diagonal_(-self.INF)\n",
    "        \n",
    "        # Messages initialization (N x N)\n",
    "        self.tilde_rho = torch.zeros((self.N, self.N), device=self.device)\n",
    "        self.tilde_eta = torch.zeros((self.N, self.N), device=self.device)\n",
    "        self.tilde_phi = torch.zeros((self.N, self.N), device=self.device)\n",
    "        \n",
    "        # Precompute Masks by Population Count\n",
    "        self.masks_by_popcount = [[] for _ in range(self.N + 1)]\n",
    "        for mask in range(self.num_states):\n",
    "            cnt = bin(mask).count('1')\n",
    "            if cnt <= self.N:\n",
    "                self.masks_by_popcount[cnt].append(mask)\n",
    "        \n",
    "        # GPU Tensor로 변환\n",
    "        self.masks_by_popcount_t = [\n",
    "            torch.tensor(m, dtype=torch.long, device=self.device) \n",
    "            for m in self.masks_by_popcount\n",
    "        ]\n",
    "\n",
    "    def _calc_lambda_original_def(self):\n",
    "        \"\"\"\n",
    "        [Gauge Fixing 제거됨]\n",
    "        PDF 식 (2.2)의 원래 정의: lambda_{it} = rho_{it}(b_{it})\n",
    "        \n",
    "        - 경로가 i를 방문한다면(b=1): rho(1) 메시지를 받음.\n",
    "        - 따라서 Trellis Score에 rho(1)에 해당하는 값을 '더해줌(+)' (Positive Feedback).\n",
    "        - 복잡한 Gauge 계수(-(N-1)/N 등)를 제거하고 Belief(tilde_rho)를 그대로 사용.\n",
    "        \"\"\"\n",
    "        # 1. Bias는 Rho (Belief) 그 자체\n",
    "        # Positive Feedback: 확률이 높은 경로에 가산점을 부여하여 강화\n",
    "        lambda_val = self.tilde_rho.clone()\n",
    "        \n",
    "        # 2. 수치 안정성(Numerical Stability)을 위한 Normalization\n",
    "        # Gauge Fixing이 아니라, 값이 무한정 커지는 것을 막기 위해 평균만 0으로 맞춤 (BP 표준 기법)\n",
    "        lambda_val -= torch.mean(lambda_val)\n",
    "        \n",
    "        return lambda_val\n",
    "\n",
    "    def _run_trellis_gpu(self):\n",
    "        \"\"\"GPU Optimized Trellis with defined Lambda\"\"\"\n",
    "        N, S, depot = self.N, self.S, self.depot\n",
    "        \n",
    "        # [변경] 원래 정의에 따른 Lambda 계산 호출\n",
    "        bias = self._calc_lambda_original_def() # (N, N)\n",
    "        \n",
    "        # --- [1] Forward (Alpha) ---\n",
    "        alpha = torch.full((self.num_states, N), -self.INF, device=self.device)\n",
    "        \n",
    "        # t=0 초기화\n",
    "        start_bias = bias[0] \n",
    "        start_scores = S[depot, :N] + start_bias \n",
    "        \n",
    "        initial_masks = (1 << torch.arange(N, device=self.device))\n",
    "        alpha[initial_masks, torch.arange(N, device=self.device)] = start_scores\n",
    "\n",
    "        # DP Loop\n",
    "        for t in range(1, N):\n",
    "            current_bias = bias[t] \n",
    "            prev_masks = self.masks_by_popcount_t[t]\n",
    "            \n",
    "            # (최적화된 Vectorized DP 로직 유지)\n",
    "            curr_scores = alpha[prev_masks, :] \n",
    "            transition_cost = S[:N, :N] + current_bias.view(1, N)\n",
    "            \n",
    "            candidates = curr_scores.unsqueeze(2) + transition_cost.unsqueeze(0)\n",
    "            \n",
    "            mask_col = prev_masks.view(-1, 1)\n",
    "            shifts = torch.arange(N, device=self.device).view(1, -1)\n",
    "            next_bit_check = (mask_col & (1 << shifts)) == 0\n",
    "            \n",
    "            valid_mask = next_bit_check.unsqueeze(1).expand(-1, N, -1)\n",
    "            candidates = torch.where(valid_mask, candidates, torch.tensor(-self.INF, device=self.device))\n",
    "            \n",
    "            new_masks_calc = mask_col | (1 << shifts)\n",
    "            src_vals = candidates.reshape(-1)\n",
    "            \n",
    "            m_idx = new_masks_calc.unsqueeze(1).expand(-1, N, -1).reshape(-1)\n",
    "            n_idx = torch.arange(N, device=self.device).view(1, 1, N).expand(len(prev_masks), N, -1).reshape(-1)\n",
    "            flat_indices = m_idx * N + n_idx\n",
    "            \n",
    "            alpha.view(-1).scatter_reduce_(0, flat_indices, src_vals, reduce='amax', include_self=True)\n",
    "\n",
    "        # --- [2] Backward (Beta) ---\n",
    "        beta = torch.full((self.num_states, N), -self.INF, device=self.device)\n",
    "        beta[self.FULL_MASK, :] = S[:N, depot]\n",
    "        \n",
    "        for t in range(N - 1, -1, -1):\n",
    "            curr_bias = bias[t]\n",
    "            next_masks = self.masks_by_popcount_t[t+1]\n",
    "            if len(next_masks) == 0: continue\n",
    "            \n",
    "            next_beta_vals = beta[next_masks, :]\n",
    "            xi_val = next_beta_vals + curr_bias.view(1, N)\n",
    "            \n",
    "            candidates = xi_val.unsqueeze(1) + S[:N, :N].unsqueeze(0)\n",
    "            \n",
    "            mask_col = next_masks.view(-1, 1)\n",
    "            shifts = torch.arange(N, device=self.device).view(1, -1)\n",
    "            has_next_node = (mask_col & (1 << shifts)) != 0\n",
    "            prev_masks_calc = mask_col ^ (1 << shifts)\n",
    "            \n",
    "            if t > 0:\n",
    "                 prev_has_j = (prev_masks_calc.unsqueeze(2) & (1 << shifts).unsqueeze(0).unsqueeze(0)) != 0\n",
    "            else:\n",
    "                 pass\n",
    "            \n",
    "            valid_mask = has_next_node.unsqueeze(1).expand(-1, N, -1)\n",
    "            if t > 0:\n",
    "                valid_mask = valid_mask & prev_has_j\n",
    "            \n",
    "            vals = torch.where(valid_mask, candidates, torch.tensor(-self.INF, device=self.device))\n",
    "            \n",
    "            p_mask_idx = prev_masks_calc.unsqueeze(1).expand(-1, N, -1).reshape(-1)\n",
    "            p_node_idx = torch.arange(N, device=self.device).view(1, N, 1).expand(len(next_masks), -1, N).reshape(-1)\n",
    "            flat_indices = p_mask_idx * N + p_node_idx\n",
    "            src_vals = vals.reshape(-1)\n",
    "            \n",
    "            beta.view(-1).scatter_reduce_(0, flat_indices, src_vals, reduce='amax', include_self=True)\n",
    "\n",
    "        # --- [3] Soft Output (Delta) ---\n",
    "        tilde_delta = torch.zeros((N, N), device=self.device)\n",
    "        \n",
    "        for t in range(N):\n",
    "            curr_masks = self.masks_by_popcount_t[t+1]\n",
    "            if len(curr_masks) == 0: continue\n",
    "            \n",
    "            a = alpha[curr_masks]\n",
    "            b = beta[curr_masks]\n",
    "            valid = (a > -self.INF/2) & (b > -self.INF/2)\n",
    "            scores = torch.where(valid, a + b, torch.tensor(-self.INF, device=self.device))\n",
    "            \n",
    "            city_max_scores = torch.max(scores, dim=0)[0]\n",
    "            \n",
    "            top2_vals, top2_idxs = torch.topk(city_max_scores, 2)\n",
    "            global_max = top2_vals[0]\n",
    "            global_second = top2_vals[1]\n",
    "            best_idx = top2_idxs[0]\n",
    "            \n",
    "            # [변경] Max-In/Max-Out 계산 시에도 Gauge Fix 계수 제거\n",
    "            # 순수하게 점수 차이(Diff)만 계산\n",
    "            # 기존 식: lam_i_for_i = -(1.0/N) * rho_t 등 -> 모두 제거\n",
    "            # Lambda(Bias)는 이미 Trellis 계산에 녹아있으므로, \n",
    "            # 여기서는 순수 Trellis Score(city_max_scores)만으로 Delta를 구함\n",
    "            \n",
    "            max_in = city_max_scores # Bias 항 제거됨\n",
    "            \n",
    "            max_out_raw = torch.full((N,), global_max, device=self.device)\n",
    "            max_out_raw[best_idx] = global_second\n",
    "            max_out = max_out_raw # Bias 항 제거됨\n",
    "            \n",
    "            diff = max_in - max_out\n",
    "            \n",
    "            diff = torch.where(max_in < -self.INF/2, torch.tensor(-self.INF, device=self.device), diff)\n",
    "            diff = torch.where(max_out < -self.INF/2, torch.tensor(self.INF, device=self.device), diff)\n",
    "            \n",
    "            tilde_delta[t] = diff\n",
    "            \n",
    "        return alpha, tilde_delta\n",
    "\n",
    "    def _run_bp_gpu(self, tilde_delta):\n",
    "        \"\"\"\n",
    "        Matrix Operations Fully on GPU\n",
    "        Constraint Satisfaction을 위한 BP 업데이트 (Doubly Stochastic)\n",
    "        \"\"\"\n",
    "        N = self.N\n",
    "        \n",
    "        # 1. Omega\n",
    "        t_omega = self.tilde_phi + tilde_delta\n",
    "        \n",
    "        # 2. Eta (Column Max)\n",
    "        vals, idxs = torch.topk(t_omega, 2, dim=0)\n",
    "        max1 = vals[0]\n",
    "        max2 = vals[1]\n",
    "        argmax = idxs[0]\n",
    "        rows = torch.arange(N, device=self.device).view(N, 1).expand(N, N)\n",
    "        is_max_pos = (rows == argmax)\n",
    "        new_eta = -torch.where(is_max_pos, max2, max1)\n",
    "        \n",
    "        # 3. Gamma\n",
    "        t_gamma = new_eta + tilde_delta\n",
    "        \n",
    "        # 4. Phi (Row Max)\n",
    "        vals_r, idxs_r = torch.topk(t_gamma, 2, dim=1)\n",
    "        max1_r = vals_r[:, 0].unsqueeze(1)\n",
    "        max2_r = vals_r[:, 1].unsqueeze(1)\n",
    "        argmax_r = idxs_r[:, 0].unsqueeze(1)\n",
    "        cols = torch.arange(N, device=self.device).view(1, N).expand(N, N)\n",
    "        is_max_pos_r = (cols == argmax_r)\n",
    "        new_phi = -torch.where(is_max_pos_r, max2_r, max1_r)\n",
    "        \n",
    "        # Update with Damping (Gradient Descent 대신 관성 사용)\n",
    "        self.tilde_eta = self.damping * self.tilde_eta + (1 - self.damping) * new_eta\n",
    "        self.tilde_phi = self.damping * self.tilde_phi + (1 - self.damping) * new_phi\n",
    "        \n",
    "        # --- Rho Update ---\n",
    "        raw_rho = self.tilde_eta + self.tilde_phi\n",
    "        \n",
    "        # [중요] Gauge Fixing 제거 후 Normalization\n",
    "        # PDF 식 (4.2)의 Double Centering은 Gauge Fix에서 나온 것이지만,\n",
    "        # Max-Sum BP에서 값이 무한정 커지는 것을 막고 행/열 제약을 맞추기 위해 \n",
    "        # 'Sinkhorn' 스타일의 Normalization은 유지하는 것이 수렴에 유리합니다.\n",
    "        # 하지만 \"Gauge Fixing을 걷어내라\"는 요청에 따라, 최소한의 수치 안정성(Mean Subtraction)만 남깁니다.\n",
    "        \n",
    "        # 1. Row Centering\n",
    "        raw_rho -= torch.mean(raw_rho, dim=1, keepdim=True)\n",
    "        # 2. Col Centering\n",
    "        raw_rho -= torch.mean(raw_rho, dim=0, keepdim=True)\n",
    "        \n",
    "        self.tilde_rho = raw_rho\n",
    "\n",
    "    def _extract_path_gpu(self, alpha):\n",
    "            path = []\n",
    "            curr_mask = self.FULL_MASK\n",
    "            \n",
    "            # S 행렬 슬라이싱 주의 (N x N)\n",
    "            final_scores = alpha[self.FULL_MASK, :] + self.S[:self.N, self.depot]\n",
    "            best_last = torch.argmax(final_scores).item()\n",
    "            best_score = final_scores[best_last].item()\n",
    "            \n",
    "            if best_score < -self.INF/2:\n",
    "                return [], self.INF\n",
    "                \n",
    "            path = [self.depot, best_last]\n",
    "            curr_node = best_last\n",
    "            curr_mask = self.FULL_MASK ^ (1 << best_last)\n",
    "            \n",
    "            for t in range(self.N - 1, 0, -1):\n",
    "                scores = alpha[curr_mask, :] + self.S[:self.N, curr_node]\n",
    "                best_prev = torch.argmax(scores).item()\n",
    "                val = scores[best_prev].item()\n",
    "                \n",
    "                if val < -self.INF/2:\n",
    "                    return [], self.INF\n",
    "                    \n",
    "                path.append(best_prev)\n",
    "                curr_node = best_prev\n",
    "                curr_mask ^= (1 << best_prev)\n",
    "                \n",
    "            path.append(self.depot)\n",
    "            path.reverse()\n",
    "            \n",
    "            dist_cpu = self.dist_matrix.cpu().numpy()\n",
    "            cost = sum(dist_cpu[path[k], path[k+1]] for k in range(len(path)-1))\n",
    "            \n",
    "            return path, cost\n",
    "\n",
    "    def solve(self):\n",
    "        best_global_path = []\n",
    "        best_global_cost = self.INF\n",
    "        \n",
    "        print(f\"Solving TSP (N={self.N}) - Original Definition (Lambda=Rho)\")\n",
    "        \n",
    "        for it in range(self.bp_iterations):\n",
    "            alpha, tilde_delta = self._run_trellis_gpu()\n",
    "            path, cost = self._extract_path_gpu(alpha)\n",
    "            \n",
    "            if cost < best_global_cost:\n",
    "                best_global_cost = cost\n",
    "                best_global_path = path\n",
    "                print(f\"[Iter {it}] Cost: {cost:.2f} (New Best!)\")\n",
    "            elif self.verbose:\n",
    "                print(f\"[Iter {it}] Cost: {cost:.2f}\")\n",
    "\n",
    "            self._run_bp_gpu(tilde_delta)\n",
    "            \n",
    "        return best_global_path, best_global_cost\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # N=14 예제 테스트\n",
    "    N_CITIES = 20\n",
    "    np.random.seed(42)\n",
    "    data = np.random.rand(N_CITIES, N_CITIES) * 100\n",
    "    np.fill_diagonal(data, 0)\n",
    "    \n",
    "    solver = TSPSolverSOVATorch(data, bp_iterations=20, damping=0.7, verbose=True)\n",
    "    path, cost = solver.solve()\n",
    "    print(\"Final Path:\", path)\n",
    "    print(\"Final Cost:\", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "57bb27c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving TSP (N=19) - Original Definition (Lambda=Rho)\n",
      "[Iter 0] Cost: 114.97 (New Best!)\n",
      "[Iter 1] Cost: 114.97\n",
      "[Iter 2] Cost: 114.97\n",
      "[Iter 3] Cost: 114.97\n",
      "[Iter 4] Cost: 114.97\n",
      "[Iter 5] Cost: 124.71\n",
      "[Iter 6] Cost: 124.71\n",
      "[Iter 7] Cost: 124.71\n",
      "[Iter 8] Cost: 124.71\n",
      "[Iter 9] Cost: 124.71\n",
      "[Iter 10] Cost: 124.71\n",
      "[Iter 11] Cost: 124.71\n",
      "[Iter 12] Cost: 124.71\n",
      "[Iter 13] Cost: 124.71\n",
      "[Iter 14] Cost: 124.71\n",
      "[Iter 15] Cost: 124.71\n",
      "[Iter 16] Cost: 124.71\n",
      "[Iter 17] Cost: 124.71\n",
      "[Iter 18] Cost: 124.71\n",
      "[Iter 19] Cost: 124.71\n",
      "Final Path: [19, 12, 7, 9, 3, 4, 5, 2, 11, 6, 18, 0, 1, 14, 10, 17, 16, 8, 15, 13, 19]\n",
      "Final Cost: 114.9748\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class TSPSolverSOVATorch:\n",
    "    def __init__(self, dist_matrix, bp_iterations=20, damping=0.7, verbose=True, device='cuda'):\n",
    "        # Device 설정\n",
    "        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # 데이터 초기화 및 Tensor 변환\n",
    "        self.dist_matrix = torch.tensor(dist_matrix, dtype=torch.float32, device=self.device)\n",
    "        self.num_nodes = self.dist_matrix.shape[0]\n",
    "        self.N = self.num_nodes - 1\n",
    "        self.depot = self.N\n",
    "        self.bp_iterations = bp_iterations\n",
    "        self.damping = damping  # BP 메시지 업데이트 시의 관성(Momentum)\n",
    "        self.verbose = verbose\n",
    "        self.INF = 1e8\n",
    "        \n",
    "        # [변경] accumulated_bias 제거 (Gradient 방식 폐기)\n",
    "        # 본래 BP 정의대로 매 iter마다 메시지를 새로 계산하여 전달합니다.\n",
    "\n",
    "        # 비트마스크 전체 크기 (2^N)\n",
    "        self.num_states = 1 << self.N\n",
    "        self.FULL_MASK = self.num_states - 1\n",
    "        \n",
    "        # S Matrix 계산 (Max - Dist)\n",
    "        max_dist = torch.max(self.dist_matrix)\n",
    "        self.S = max_dist - self.dist_matrix\n",
    "        self.S.fill_diagonal_(-self.INF)\n",
    "        \n",
    "        # Messages initialization (N x N)\n",
    "        self.tilde_rho = torch.zeros((self.N, self.N), device=self.device)\n",
    "        self.tilde_eta = torch.zeros((self.N, self.N), device=self.device)\n",
    "        self.tilde_phi = torch.zeros((self.N, self.N), device=self.device)\n",
    "        \n",
    "        # Precompute Masks by Population Count\n",
    "        self.masks_by_popcount = [[] for _ in range(self.N + 1)]\n",
    "        for mask in range(self.num_states):\n",
    "            cnt = bin(mask).count('1')\n",
    "            if cnt <= self.N:\n",
    "                self.masks_by_popcount[cnt].append(mask)\n",
    "        \n",
    "        # GPU Tensor로 변환\n",
    "        self.masks_by_popcount_t = [\n",
    "            torch.tensor(m, dtype=torch.long, device=self.device) \n",
    "            for m in self.masks_by_popcount\n",
    "        ]\n",
    "\n",
    "    def _calc_lambda_original_def(self):\n",
    "        rho_tilde = self.tilde_rho  # (i, t)\n",
    "        N = self.N\n",
    "\n",
    "        # Σ_i ρ̃\n",
    "        col_sum = rho_tilde.sum(dim=0, keepdim=True)\n",
    "\n",
    "        # PDF 구조 유지\n",
    "        bias_per_city = (N - 1)/N * col_sum - rho_tilde  # (a, t)\n",
    "        bias = bias_per_city.T  # (t, a)\n",
    "\n",
    "        # (A) 안정성 개선: λ scale down\n",
    "        c = 0.2\n",
    "        bias = c * bias\n",
    "\n",
    "        # (B) 안정성 개선: damping (low-pass filtering)\n",
    "        if not hasattr(self, \"lambda_eff\"):\n",
    "            self.lambda_eff = torch.zeros_like(bias)\n",
    "\n",
    "        gamma = 0.9\n",
    "        self.lambda_eff = gamma * self.lambda_eff + (1 - gamma) * bias\n",
    "\n",
    "        # 발산 방지용 mean subtraction\n",
    "        self.lambda_eff = self.lambda_eff - self.lambda_eff.mean()\n",
    "\n",
    "        return self.lambda_eff\n",
    "\n",
    "\n",
    "\n",
    "    def _run_trellis_gpu(self):\n",
    "        \"\"\"GPU Optimized Trellis with defined Lambda\"\"\"\n",
    "        N, S, depot = self.N, self.S, self.depot\n",
    "        \n",
    "        # [변경] 원래 정의에 따른 Lambda 계산 호출\n",
    "        bias = self._calc_lambda_original_def() # (N, N)\n",
    "        \n",
    "        # --- [1] Forward (Alpha) ---\n",
    "        alpha = torch.full((self.num_states, N), -self.INF, device=self.device)\n",
    "        \n",
    "        # t=0 초기화\n",
    "        start_bias = bias[0] \n",
    "        start_scores = S[depot, :N] + start_bias \n",
    "        \n",
    "        initial_masks = (1 << torch.arange(N, device=self.device))\n",
    "        alpha[initial_masks, torch.arange(N, device=self.device)] = start_scores\n",
    "\n",
    "        # DP Loop\n",
    "        for t in range(1, N):\n",
    "            current_bias = bias[t] \n",
    "            prev_masks = self.masks_by_popcount_t[t]\n",
    "            \n",
    "            # (최적화된 Vectorized DP 로직 유지)\n",
    "            curr_scores = alpha[prev_masks, :] \n",
    "            transition_cost = S[:N, :N] + current_bias.view(1, N)\n",
    "            \n",
    "            candidates = curr_scores.unsqueeze(2) + transition_cost.unsqueeze(0)\n",
    "            \n",
    "            mask_col = prev_masks.view(-1, 1)\n",
    "            shifts = torch.arange(N, device=self.device).view(1, -1)\n",
    "            next_bit_check = (mask_col & (1 << shifts)) == 0\n",
    "            \n",
    "            valid_mask = next_bit_check.unsqueeze(1).expand(-1, N, -1)\n",
    "            candidates = torch.where(valid_mask, candidates, torch.tensor(-self.INF, device=self.device))\n",
    "            \n",
    "            new_masks_calc = mask_col | (1 << shifts)\n",
    "            src_vals = candidates.reshape(-1)\n",
    "            \n",
    "            m_idx = new_masks_calc.unsqueeze(1).expand(-1, N, -1).reshape(-1)\n",
    "            n_idx = torch.arange(N, device=self.device).view(1, 1, N).expand(len(prev_masks), N, -1).reshape(-1)\n",
    "            flat_indices = m_idx * N + n_idx\n",
    "            \n",
    "            alpha.view(-1).scatter_reduce_(0, flat_indices, src_vals, reduce='amax', include_self=True)\n",
    "\n",
    "        # --- [2] Backward (Beta) ---\n",
    "        beta = torch.full((self.num_states, N), -self.INF, device=self.device)\n",
    "        beta[self.FULL_MASK, :] = S[:N, depot]\n",
    "        \n",
    "        for t in range(N - 1, -1, -1):\n",
    "            curr_bias = bias[t]\n",
    "            next_masks = self.masks_by_popcount_t[t+1]\n",
    "            if len(next_masks) == 0: continue\n",
    "            \n",
    "            next_beta_vals = beta[next_masks, :]\n",
    "            xi_val = next_beta_vals + curr_bias.view(1, N)\n",
    "            \n",
    "            candidates = xi_val.unsqueeze(1) + S[:N, :N].unsqueeze(0)\n",
    "            \n",
    "            mask_col = next_masks.view(-1, 1)\n",
    "            shifts = torch.arange(N, device=self.device).view(1, -1)\n",
    "            has_next_node = (mask_col & (1 << shifts)) != 0\n",
    "            prev_masks_calc = mask_col ^ (1 << shifts)\n",
    "            \n",
    "            if t > 0:\n",
    "                 prev_has_j = (prev_masks_calc.unsqueeze(2) & (1 << shifts).unsqueeze(0).unsqueeze(0)) != 0\n",
    "            else:\n",
    "                 pass\n",
    "            \n",
    "            valid_mask = has_next_node.unsqueeze(1).expand(-1, N, -1)\n",
    "            if t > 0:\n",
    "                valid_mask = valid_mask & prev_has_j\n",
    "            \n",
    "            vals = torch.where(valid_mask, candidates, torch.tensor(-self.INF, device=self.device))\n",
    "            \n",
    "            p_mask_idx = prev_masks_calc.unsqueeze(1).expand(-1, N, -1).reshape(-1)\n",
    "            p_node_idx = torch.arange(N, device=self.device).view(1, N, 1).expand(len(next_masks), -1, N).reshape(-1)\n",
    "            flat_indices = p_mask_idx * N + p_node_idx\n",
    "            src_vals = vals.reshape(-1)\n",
    "            \n",
    "            beta.view(-1).scatter_reduce_(0, flat_indices, src_vals, reduce='amax', include_self=True)\n",
    "\n",
    "        # --- [3] Soft Output (Delta) ---\n",
    "        tilde_delta = torch.zeros((N, N), device=self.device)\n",
    "        \n",
    "        for t in range(N):\n",
    "            curr_masks = self.masks_by_popcount_t[t+1]\n",
    "            if len(curr_masks) == 0: continue\n",
    "            \n",
    "            a = alpha[curr_masks]\n",
    "            b = beta[curr_masks]\n",
    "            valid = (a > -self.INF/2) & (b > -self.INF/2)\n",
    "            scores = torch.where(valid, a + b, torch.tensor(-self.INF, device=self.device))\n",
    "            \n",
    "            city_max_scores = torch.max(scores, dim=0)[0]\n",
    "            \n",
    "            top2_vals, top2_idxs = torch.topk(city_max_scores, 2)\n",
    "            global_max = top2_vals[0]\n",
    "            global_second = top2_vals[1]\n",
    "            best_idx = top2_idxs[0]\n",
    "            \n",
    "            # [변경] Max-In/Max-Out 계산 시에도 Gauge Fix 계수 제거\n",
    "            # 순수하게 점수 차이(Diff)만 계산\n",
    "            # 기존 식: lam_i_for_i = -(1.0/N) * rho_t 등 -> 모두 제거\n",
    "            # Lambda(Bias)는 이미 Trellis 계산에 녹아있으므로, \n",
    "            # 여기서는 순수 Trellis Score(city_max_scores)만으로 Delta를 구함\n",
    "            \n",
    "            max_in = city_max_scores # Bias 항 제거됨\n",
    "            \n",
    "            max_out_raw = torch.full((N,), global_max, device=self.device)\n",
    "            max_out_raw[best_idx] = global_second\n",
    "            max_out = max_out_raw # Bias 항 제거됨\n",
    "            \n",
    "            diff = max_in - max_out\n",
    "            \n",
    "            diff = torch.where(max_in < -self.INF/2, torch.tensor(-self.INF, device=self.device), diff)\n",
    "            diff = torch.where(max_out < -self.INF/2, torch.tensor(self.INF, device=self.device), diff)\n",
    "            \n",
    "            tilde_delta[t] = diff\n",
    "            \n",
    "        return alpha, tilde_delta\n",
    "\n",
    "    def _run_bp_gpu(self, tilde_delta):\n",
    "        \"\"\"\n",
    "        Matrix Operations Fully on GPU\n",
    "        Constraint Satisfaction을 위한 BP 업데이트 (Doubly Stochastic)\n",
    "        \"\"\"\n",
    "        N = self.N\n",
    "        \n",
    "        # 1. Omega\n",
    "        t_omega = self.tilde_phi + tilde_delta\n",
    "        \n",
    "        # 2. Eta (Column Max)\n",
    "        vals, idxs = torch.topk(t_omega, 2, dim=0)\n",
    "        max1 = vals[0]\n",
    "        max2 = vals[1]\n",
    "        argmax = idxs[0]\n",
    "        rows = torch.arange(N, device=self.device).view(N, 1).expand(N, N)\n",
    "        is_max_pos = (rows == argmax)\n",
    "        new_eta = -torch.where(is_max_pos, max2, max1)\n",
    "        \n",
    "        # 3. Gamma\n",
    "        t_gamma = new_eta + tilde_delta\n",
    "        \n",
    "        # 4. Phi (Row Max)\n",
    "        vals_r, idxs_r = torch.topk(t_gamma, 2, dim=1)\n",
    "        max1_r = vals_r[:, 0].unsqueeze(1)\n",
    "        max2_r = vals_r[:, 1].unsqueeze(1)\n",
    "        argmax_r = idxs_r[:, 0].unsqueeze(1)\n",
    "        cols = torch.arange(N, device=self.device).view(1, N).expand(N, N)\n",
    "        is_max_pos_r = (cols == argmax_r)\n",
    "        new_phi = -torch.where(is_max_pos_r, max2_r, max1_r)\n",
    "        \n",
    "        # Update with Damping (Gradient Descent 대신 관성 사용)\n",
    "        self.tilde_eta = self.damping * self.tilde_eta + (1 - self.damping) * new_eta\n",
    "        self.tilde_phi = self.damping * self.tilde_phi + (1 - self.damping) * new_phi\n",
    "        \n",
    "        # --- Rho Update ---\n",
    "        raw_rho = self.tilde_eta + self.tilde_phi\n",
    "        \n",
    "        # [중요] Gauge Fixing 제거 후 Normalization\n",
    "        # PDF 식 (4.2)의 Double Centering은 Gauge Fix에서 나온 것이지만,\n",
    "        # Max-Sum BP에서 값이 무한정 커지는 것을 막고 행/열 제약을 맞추기 위해 \n",
    "        # 'Sinkhorn' 스타일의 Normalization은 유지하는 것이 수렴에 유리합니다.\n",
    "        # 하지만 \"Gauge Fixing을 걷어내라\"는 요청에 따라, 최소한의 수치 안정성(Mean Subtraction)만 남깁니다.\n",
    "        \n",
    "        # 1. Row Centering\n",
    "        raw_rho -= torch.mean(raw_rho, dim=1, keepdim=True)\n",
    "        # 2. Col Centering\n",
    "        raw_rho -= torch.mean(raw_rho, dim=0, keepdim=True)\n",
    "        \n",
    "        self.tilde_rho = raw_rho\n",
    "\n",
    "    def _extract_path_gpu(self, alpha):\n",
    "            path = []\n",
    "            curr_mask = self.FULL_MASK\n",
    "            \n",
    "            # S 행렬 슬라이싱 주의 (N x N)\n",
    "            final_scores = alpha[self.FULL_MASK, :] + self.S[:self.N, self.depot]\n",
    "            best_last = torch.argmax(final_scores).item()\n",
    "            best_score = final_scores[best_last].item()\n",
    "            \n",
    "            if best_score < -self.INF/2:\n",
    "                return [], self.INF\n",
    "                \n",
    "            path = [self.depot, best_last]\n",
    "            curr_node = best_last\n",
    "            curr_mask = self.FULL_MASK ^ (1 << best_last)\n",
    "            \n",
    "            for t in range(self.N - 1, 0, -1):\n",
    "                scores = alpha[curr_mask, :] + self.S[:self.N, curr_node]\n",
    "                best_prev = torch.argmax(scores).item()\n",
    "                val = scores[best_prev].item()\n",
    "                \n",
    "                if val < -self.INF/2:\n",
    "                    return [], self.INF\n",
    "                    \n",
    "                path.append(best_prev)\n",
    "                curr_node = best_prev\n",
    "                curr_mask ^= (1 << best_prev)\n",
    "                \n",
    "            path.append(self.depot)\n",
    "            path.reverse()\n",
    "            \n",
    "            dist_cpu = self.dist_matrix.cpu().numpy()\n",
    "            cost = sum(dist_cpu[path[k], path[k+1]] for k in range(len(path)-1))\n",
    "            \n",
    "            return path, cost\n",
    "\n",
    "    def solve(self):\n",
    "        best_global_path = []\n",
    "        best_global_cost = self.INF\n",
    "        \n",
    "        print(f\"Solving TSP (N={self.N}) - Original Definition (Lambda=Rho)\")\n",
    "        \n",
    "        for it in range(self.bp_iterations):\n",
    "            alpha, tilde_delta = self._run_trellis_gpu()\n",
    "            path, cost = self._extract_path_gpu(alpha)\n",
    "            \n",
    "            if cost < best_global_cost:\n",
    "                best_global_cost = cost\n",
    "                best_global_path = path\n",
    "                print(f\"[Iter {it}] Cost: {cost:.2f} (New Best!)\")\n",
    "            elif self.verbose:\n",
    "                print(f\"[Iter {it}] Cost: {cost:.2f}\")\n",
    "\n",
    "            self._run_bp_gpu(tilde_delta)\n",
    "            \n",
    "        return best_global_path, best_global_cost\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # N=14 예제 테스트\n",
    "    N_CITIES = 20\n",
    "    np.random.seed(10)\n",
    "    data = np.random.rand(N_CITIES, N_CITIES) * 100\n",
    "    np.fill_diagonal(data, 0)\n",
    "    \n",
    "    solver = TSPSolverSOVATorch(data, bp_iterations=20, damping=0.7, verbose=True)\n",
    "    path, cost = solver.solve()\n",
    "    print(\"Final Path:\", path)\n",
    "    print(\"Final Cost:\", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d8742fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving TSP (N=9) with Alpha Visualization\n",
      "[Iter 0] Cost: 1.34 (New Best!) | Path: [9, 3, 0, 5, 2, 1, 7, 4, 8, 6, 9]\n",
      "[Iter 1] Cost: 1.37 | Path: [9, 5, 2, 1, 0, 3, 6, 8, 4, 7, 9]\n",
      "[Iter 2] Cost: 1.34 | Path: [9, 3, 0, 5, 2, 1, 7, 4, 8, 6, 9]\n",
      "[Iter 3] Cost: 1.50 | Path: [9, 1, 7, 4, 8, 6, 2, 3, 0, 5, 9]\n",
      "[Iter 4] Cost: 1.34 | Path: [9, 5, 0, 3, 6, 8, 4, 2, 1, 7, 9]\n",
      "[Iter 5] Cost: 1.44 | Path: [9, 3, 6, 8, 7, 4, 2, 1, 0, 5, 9]\n",
      "[Iter 6] Cost: 1.60 | Path: [9, 4, 2, 1, 0, 3, 6, 8, 7, 5, 9]\n",
      "[Iter 7] Cost: 1.34 | Path: [9, 3, 0, 5, 2, 1, 7, 4, 8, 6, 9]\n",
      "[Iter 8] Cost: 1.50 | Path: [9, 1, 7, 4, 8, 6, 2, 3, 0, 5, 9]\n",
      "[Iter 9] Cost: 1.37 | Path: [9, 5, 2, 1, 0, 3, 6, 8, 4, 7, 9]\n",
      "[Iter 10] Cost: 1.70 | Path: [9, 4, 8, 7, 5, 2, 1, 0, 3, 6, 9]\n",
      "[Iter 11] Cost: 1.44 | Path: [9, 3, 6, 8, 7, 4, 2, 1, 0, 5, 9]\n",
      "[Iter 12] Cost: 1.60 | Path: [9, 4, 2, 1, 0, 3, 6, 8, 7, 5, 9]\n",
      "[Iter 13] Cost: 1.60 | Path: [9, 4, 2, 1, 0, 3, 6, 8, 7, 5, 9]\n",
      "[Iter 14] Cost: 1.50 | Path: [9, 1, 7, 4, 8, 6, 2, 3, 0, 5, 9]\n",
      "[Iter 15] Cost: 1.34 | Path: [9, 3, 0, 5, 2, 1, 7, 4, 8, 6, 9]\n",
      "[Iter 16] Cost: 1.71 | Path: [9, 5, 7, 8, 4, 2, 1, 0, 3, 6, 9]\n",
      "[Iter 17] Cost: 1.71 | Path: [9, 5, 7, 8, 4, 2, 1, 0, 3, 6, 9]\n",
      "[Iter 18] Cost: 1.71 | Path: [9, 5, 7, 8, 4, 2, 1, 0, 3, 6, 9]\n",
      "[Iter 19] Cost: 1.71 | Path: [9, 5, 7, 8, 4, 2, 1, 0, 3, 6, 9]\n",
      "Final Path: [9, 3, 0, 5, 2, 1, 7, 4, 8, 6, 9]\n",
      "Final Cost: 1.337004520531317\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class TSPSolverSOVA:\n",
    "    def __init__(self, dist_matrix, bp_iterations=20, damping=0.7, verbose=True):\n",
    "        self.dist_matrix = np.array(dist_matrix)\n",
    "        self.num_nodes = self.dist_matrix.shape[0]\n",
    "        self.N = self.num_nodes - 1\n",
    "        self.depot = self.N\n",
    "        self.bp_iterations = bp_iterations\n",
    "        self.damping = damping\n",
    "        self.verbose = verbose\n",
    "        self.INF = 1e12\n",
    "        self.FULL_MASK = (1 << self.N) - 1\n",
    "        \n",
    "        max_dist = np.max(self.dist_matrix)\n",
    "        self.S = max_dist - self.dist_matrix\n",
    "        np.fill_diagonal(self.S, -self.INF) \n",
    "        \n",
    "        # Messages initialization\n",
    "        self.tilde_rho = np.zeros((self.N, self.N))\n",
    "        self.tilde_eta = np.zeros((self.N, self.N))\n",
    "        self.tilde_phi = np.zeros((self.N, self.N))\n",
    "\n",
    "    def log(self, msg):\n",
    "        if self.verbose:\n",
    "            print(msg)\n",
    "\n",
    "    def _calc_lambda_sum_bias(self):\n",
    "        \"\"\"PDF Eq (1.2) Sum(lambda) 계산 (Beta 역할)\"\"\"\n",
    "        N = self.N\n",
    "        sum_rho_t = np.sum(self.tilde_rho, axis=1, keepdims=True)\n",
    "        lambda_sum_bias = -self.tilde_rho + ((N - 1) / N) * sum_rho_t\n",
    "        return lambda_sum_bias\n",
    "\n",
    "    def _run_trellis(self):\n",
    "        N, S, depot = self.N, self.S, self.depot\n",
    "        bias = self._calc_lambda_sum_bias()\n",
    "        \n",
    "        # --- [1] Forward (Alpha) ---\n",
    "        alpha = [defaultdict(lambda: -self.INF) for _ in range(N + 1)]\n",
    "        alpha[0][(0, depot)] = 0.0\n",
    "        \n",
    "        # 최적화: 루프 내 변수 접근 최소화\n",
    "        for t in range(N):\n",
    "            current_bias = bias[t]\n",
    "            next_alpha = alpha[t+1]\n",
    "            # items() 복사 오버헤드 방지\n",
    "            for state, prev_score in alpha[t].items():\n",
    "                if prev_score < -self.INF / 2: continue\n",
    "                    \n",
    "                mask, prev_node = state\n",
    "                \n",
    "                # 방문 가능한 노드만 순회 (비트 연산 최적화 가능하지만 가독성 유지)\n",
    "                # N이 작으므로 range(N) 루프는 빠름\n",
    "                for i in range(N):\n",
    "                    if not (mask & (1 << i)):\n",
    "                        new_mask = mask | (1 << i)\n",
    "                        val = prev_score + S[prev_node, i] + current_bias[i]\n",
    "                        \n",
    "                        if val > next_alpha[(new_mask, i)]:\n",
    "                            next_alpha[(new_mask, i)] = val\n",
    "\n",
    "        # --- [2] Backward (Beta) ---\n",
    "        # Beta는 로직 동일, 코드 생략 없이 최적화만 적용\n",
    "        beta = [defaultdict(lambda: -self.INF) for _ in range(N + 2)]\n",
    "        final_mask = self.FULL_MASK\n",
    "        \n",
    "        for i in range(N):\n",
    "            beta[N][(final_mask, i)] = S[i, depot]\n",
    "            \n",
    "        for t in range(N - 1, -1, -1):\n",
    "            curr_bias = bias[t]\n",
    "            curr_beta = beta[t]\n",
    "            \n",
    "            for next_state, next_beta_val in beta[t+1].items():\n",
    "                if next_beta_val < -self.INF / 2: continue\n",
    "                next_mask, next_node = next_state\n",
    "                \n",
    "                # Xi 값 미리 계산\n",
    "                xi_val = next_beta_val + curr_bias[next_node]\n",
    "                \n",
    "                prev_mask = next_mask & ~(1 << next_node)\n",
    "                \n",
    "                # 후보군 추출\n",
    "                if prev_mask == 0:\n",
    "                    cands = [depot]\n",
    "                else:\n",
    "                    cands = [j for j in range(N) if prev_mask & (1 << j)]\n",
    "                    \n",
    "                for prev in cands:\n",
    "                    val = S[prev, next_node] + xi_val\n",
    "                    state = (prev_mask, prev)\n",
    "                    if val > curr_beta[state]:\n",
    "                        curr_beta[state] = val\n",
    "\n",
    "        # --- [3] Soft Output (Delta) 최적화 (핵심!) ---\n",
    "        # 기존: O(N^2 * States) -> 최적화: O(States + N^2)\n",
    "        tilde_delta = np.zeros((N, N))\n",
    "        \n",
    "        for t in range(N):\n",
    "            # 1. 해당 시간 t의 도시별 최대 점수(City Max Scores)를 한 번의 루프로 수집\n",
    "            city_max_scores = np.full(N, -self.INF)\n",
    "            \n",
    "            # alpha와 beta가 존재하는 상태만 빠르게 스캔\n",
    "            for state, f_score in alpha[t+1].items():\n",
    "                if f_score < -self.INF / 2: continue\n",
    "                if state in beta[t+1]:\n",
    "                    b_score = beta[t+1][state]\n",
    "                    if b_score > -self.INF / 2:\n",
    "                        total = f_score + b_score\n",
    "                        city = state[1]\n",
    "                        if total > city_max_scores[city]:\n",
    "                            city_max_scores[city] = total\n",
    "            \n",
    "            # 2. '자기 자신 제외 최대값'을 구하기 위한 전처리\n",
    "            # 전체 최대값과 두 번째 최대값을 찾음\n",
    "            sorted_indices = np.argsort(city_max_scores)[::-1] # 내림차순 정렬 인덱스\n",
    "            best_idx = sorted_indices[0]\n",
    "            second_best_idx = sorted_indices[1]\n",
    "            \n",
    "            global_max = city_max_scores[best_idx]\n",
    "            global_second = city_max_scores[second_best_idx]\n",
    "\n",
    "            # 3. Delta 계산 (벡터화)\n",
    "            # lam_i_for_i = -1/N * rho\n",
    "            # lam_i_for_j = (N-1)/N * rho\n",
    "            rho_t = self.tilde_rho[t]\n",
    "            lam_i_for_i = -(1.0/N) * rho_t\n",
    "            lam_i_for_j = ((N-1.0)/N) * rho_t\n",
    "            \n",
    "            # max_in: 내가 선택된 경우의 최대 점수\n",
    "            max_in = city_max_scores - lam_i_for_i\n",
    "            \n",
    "            # max_out: 내가 선택되지 않았을 때(다른 도시 중) 최대 점수\n",
    "            # 내가 1등이면 -> 2등 점수 사용, 내가 1등 아니면 -> 1등 점수 사용\n",
    "            max_out_raw = np.full(N, global_max)\n",
    "            max_out_raw[best_idx] = global_second # 1등 자리는 2등 점수로 교체\n",
    "            \n",
    "            max_out = max_out_raw - lam_i_for_j\n",
    "            \n",
    "            # 최종 차이 계산 (Inf 처리 포함)\n",
    "            diff = np.where(max_in < -self.INF/2, -self.INF,\n",
    "                            np.where(max_out < -self.INF/2, self.INF, max_in - max_out))\n",
    "            \n",
    "            tilde_delta[t] = diff\n",
    "                \n",
    "        return alpha, tilde_delta\n",
    "\n",
    "    def _run_bp(self, tilde_delta):\n",
    "        \"\"\"\n",
    "        Numpy Broadcasting을 이용한 BP 완전 최적화\n",
    "        O(N^3) -> O(N^2)\n",
    "        \"\"\"\n",
    "        N = self.N\n",
    "        \n",
    "        # 1. Omega 계산\n",
    "        t_omega = self.tilde_phi + tilde_delta\n",
    "        \n",
    "        # 2. Eta 계산 (Row 방향 Max Excluding Self)\n",
    "        # 각 열(Column)에서 특정 행(t)을 제외한 최대값 구하기\n",
    "        # - 전체 최대값(max1)과 두번째 최대값(max2)을 구해서 처리\n",
    "        col_max_idx = np.argmax(t_omega, axis=0) # 각 열의 최대값 위치(행 인덱스)\n",
    "        col_max_val = np.max(t_omega, axis=0)    # 각 열의 최대값\n",
    "        \n",
    "        # 두 번째 최대값을 구하기 위해 최대값 위치를 -INF로 잠시 변경\n",
    "        temp_omega = t_omega.copy()\n",
    "        for c in range(N):\n",
    "            temp_omega[col_max_idx[c], c] = -self.INF\n",
    "        col_second_max = np.max(temp_omega, axis=0)\n",
    "        \n",
    "        # new_eta 구성\n",
    "        # t가 최대값 위치가 아니면 -> 최대값 사용\n",
    "        # t가 최대값 위치면 -> 두 번째 최대값 사용\n",
    "        new_eta = np.zeros((N, N))\n",
    "        for t in range(N):\n",
    "            # t행이 해당 열(c)의 최대값 위치인지 체크\n",
    "            is_max_pos = (col_max_idx == t)\n",
    "            # True면 second_max, False면 max_val\n",
    "            new_eta[t] = np.where(is_max_pos, col_second_max, col_max_val)\n",
    "        new_eta = -new_eta # 부호 반전\n",
    "\n",
    "        # 3. Gamma 계산\n",
    "        t_gamma = new_eta + tilde_delta\n",
    "        \n",
    "        # 4. Phi 계산 (Col 방향 Max Excluding Self)\n",
    "        # 이번엔 각 행(Row)에서 특정 열(i)을 제외한 최대값\n",
    "        row_max_idx = np.argmax(t_gamma, axis=1)\n",
    "        row_max_val = np.max(t_gamma, axis=1)\n",
    "        \n",
    "        temp_gamma = t_gamma.copy()\n",
    "        for r in range(N):\n",
    "            temp_gamma[r, row_max_idx[r]] = -self.INF\n",
    "        row_second_max = np.max(temp_gamma, axis=1)\n",
    "        \n",
    "        new_phi = np.zeros((N, N))\n",
    "        for i in range(N):\n",
    "            is_max_pos = (row_max_idx == i)\n",
    "            # 전치(Transpose) 주의: new_phi[t, i] 이므로 t 루프 대신 열벡터 연산\n",
    "            # 여기서는 이중 루프 없이 브로드캐스팅을 위해 전치 사용이 헷갈릴 수 있으므로 \n",
    "            # 단순하게 열 단위 할당\n",
    "            new_phi[:, i] = np.where(row_max_idx == i, row_second_max, row_max_val)\n",
    "            \n",
    "        new_phi = -new_phi # 부호 반전\n",
    "        \n",
    "        # Update with Damping\n",
    "        self.tilde_eta = self.damping * self.tilde_eta + (1 - self.damping) * new_eta\n",
    "        self.tilde_phi = self.damping * self.tilde_phi + (1 - self.damping) * new_phi\n",
    "        self.tilde_rho = self.tilde_eta + self.tilde_phi\n",
    "\n",
    "    def _extract_path(self, alpha):\n",
    "        path = []\n",
    "        curr_mask = self.FULL_MASK\n",
    "        best_score = -self.INF\n",
    "        best_last = -1\n",
    "\n",
    "        # 1) 마지막 도시 선택\n",
    "        for i in range(self.N):\n",
    "            state = (curr_mask, i)\n",
    "            if state in alpha[self.N]:\n",
    "                score = alpha[self.N][state] + self.S[i, self.depot]\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_last = i\n",
    "\n",
    "        if best_last == -1:\n",
    "            return [], self.INF\n",
    "\n",
    "        # 초기 상태\n",
    "        path = [self.depot, best_last]\n",
    "        curr_node = best_last\n",
    "        curr_mask = self.FULL_MASK ^ (1 << best_last)\n",
    "\n",
    "        # 2) 역추적\n",
    "        for t in range(self.N - 1, 0, -1):\n",
    "\n",
    "            cands = [j for j in range(self.N) if curr_mask & (1 << j)]  # depot 제거\n",
    "\n",
    "            best_prev = -1\n",
    "            best_val = -self.INF\n",
    "\n",
    "            for prev in cands:\n",
    "                state = (curr_mask, prev)\n",
    "                if state in alpha[t]:\n",
    "                    val = alpha[t][state] + self.S[prev, curr_node]   # bias 제거 (이미 α에 포함됨)\n",
    "\n",
    "                    if val > best_val:\n",
    "                        best_val = val\n",
    "                        best_prev = prev\n",
    "\n",
    "            if best_prev == -1:\n",
    "                print(f\"Traceback failed at t={t}\")\n",
    "                return [], self.INF\n",
    "\n",
    "            path.append(best_prev)\n",
    "            curr_node = best_prev\n",
    "            curr_mask ^= (1 << best_prev)\n",
    "\n",
    "        path.append(self.depot)\n",
    "        path.reverse()\n",
    "\n",
    "        # cost 계산\n",
    "        cost = sum(self.dist_matrix[path[k], path[k+1]] for k in range(len(path)-1))\n",
    "        return path, cost\n",
    "\n",
    "    def solve(self):\n",
    "        best_global_path = []\n",
    "        best_global_cost = self.INF\n",
    "        \n",
    "        print(f\"Solving TSP (N={self.N}) with Alpha Visualization\")\n",
    "        \n",
    "        for it in range(self.bp_iterations):\n",
    "            alpha, tilde_delta = self._run_trellis()\n",
    "            path, cost = self._extract_path(alpha)\n",
    "            \n",
    "            if cost < best_global_cost:\n",
    "                best_global_cost = cost\n",
    "                best_global_path = path\n",
    "                print(f\"[Iter {it}] Cost: {cost:.2f} (New Best!) | Path: {path}\")\n",
    "            else:\n",
    "                if self.verbose:\n",
    "                     print(f\"[Iter {it}] Cost: {cost:.2f} | Path: {path}\")\n",
    "\n",
    "            self._run_bp(tilde_delta)\n",
    "            \n",
    "        return best_global_path, best_global_cost\n",
    "    \n",
    "\n",
    "# --- 사용 예시 ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 임의의 거리 행렬 생성 (5x5)\n",
    "    # 실제 데이터는 파일에서 로드하거나 더 큰 N 사용\n",
    "    N_CITIES = 10\n",
    "    data = np.random.rand(N_CITIES, N_CITIES)\n",
    "    # TSP는 대칭이 아니어도 되지만, 보통 대각선 0\n",
    "    np.fill_diagonal(data, 0)\n",
    "    \n",
    "    solver = TSPSolverSOVA(data, bp_iterations=20, verbose=True)\n",
    "    path, cost = solver.solve()\n",
    "    print(\"Final Path:\", path)\n",
    "    print(\"Final Cost:\", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c13b298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving TSP (N=9) with Alpha Visualization\n",
      "[Iter 0] Cost: 2.18 (New Best!) | Path: [9, 1, 4, 0, 8, 7, 6, 2, 5, 3, 9]\n",
      "[Iter 1] Cost: 2.26 | Path: [9, 2, 1, 5, 8, 0, 3, 4, 6, 7, 9]\n",
      "[Iter 2] Cost: 2.20 | Path: [9, 1, 4, 6, 7, 2, 5, 8, 0, 3, 9]\n",
      "[Iter 3] Cost: 2.35 | Path: [9, 7, 6, 2, 1, 5, 8, 0, 3, 4, 9]\n",
      "[Iter 4] Cost: 2.35 | Path: [9, 7, 6, 2, 1, 5, 8, 0, 3, 4, 9]\n",
      "[Iter 5] Cost: 2.26 | Path: [9, 2, 1, 5, 8, 0, 3, 4, 6, 7, 9]\n",
      "[Iter 6] Cost: 2.36 | Path: [9, 0, 8, 7, 6, 2, 1, 5, 3, 4, 9]\n",
      "[Iter 7] Cost: 2.36 | Path: [9, 3, 6, 7, 2, 1, 5, 8, 0, 4, 9]\n",
      "[Iter 8] Cost: 2.36 | Path: [9, 0, 8, 7, 6, 2, 1, 5, 3, 4, 9]\n",
      "[Iter 9] Cost: 2.44 | Path: [9, 3, 8, 0, 2, 1, 5, 6, 7, 4, 9]\n",
      "Final Path: [9, 1, 4, 0, 8, 7, 6, 2, 5, 3, 9]\n",
      "Final Cost: 2.1835786372086643\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class TSPSolverSOVA:\n",
    "    def __init__(self, dist_matrix, bp_iterations=20, damping=0.7, verbose=True):\n",
    "        self.dist_matrix = np.array(dist_matrix)\n",
    "        self.num_nodes = self.dist_matrix.shape[0]\n",
    "        self.N = self.num_nodes - 1\n",
    "        self.depot = self.N\n",
    "        self.bp_iterations = bp_iterations\n",
    "        self.damping = damping\n",
    "        self.verbose = verbose\n",
    "        self.INF = 1e12\n",
    "        self.FULL_MASK = (1 << self.N) - 1\n",
    "        \n",
    "        max_dist = np.max(self.dist_matrix)\n",
    "        self.S = max_dist - self.dist_matrix\n",
    "        np.fill_diagonal(self.S, -self.INF) \n",
    "        \n",
    "        # Messages initialization\n",
    "        self.tilde_rho = np.zeros((self.N, self.N))\n",
    "        self.tilde_eta = np.zeros((self.N, self.N))\n",
    "        self.tilde_phi = np.zeros((self.N, self.N))\n",
    "\n",
    "    def log(self, msg):\n",
    "        if self.verbose:\n",
    "            print(msg)\n",
    "\n",
    "    def _calc_lambda_sum_bias(self):\n",
    "        \"\"\"PDF Eq (1.2) Sum(lambda) 계산 (Beta 역할)\"\"\"\n",
    "        N = self.N\n",
    "        sum_rho_t = np.sum(self.tilde_rho, axis=1, keepdims=True)\n",
    "        lambda_sum_bias = -self.tilde_rho + ((N - 1) / N) * sum_rho_t\n",
    "        return lambda_sum_bias\n",
    "\n",
    "    def _run_trellis(self):\n",
    "        N, S, depot = self.N, self.S, self.depot\n",
    "        bias = self._calc_lambda_sum_bias()\n",
    "        \n",
    "        # --- Forward (Alpha) ---\n",
    "        alpha = [defaultdict(lambda: -self.INF) for _ in range(N + 1)]\n",
    "        alpha[0][(0, depot)] = 0.0\n",
    "        \n",
    "        for t in range(N):\n",
    "            current_bias = bias[t]\n",
    "            for state, prev_score in alpha[t].items():\n",
    "                if prev_score < -self.INF / 2: continue\n",
    "                    \n",
    "                mask, prev_node = state\n",
    "                for i in range(N):\n",
    "                    if not (mask & (1 << i)):\n",
    "                        new_mask = mask | (1 << i)\n",
    "                        # Alpha Update: 이전 점수 + 거리 점수 + 제약 조건(Bias)\n",
    "                        val = prev_score + S[prev_node, i] + current_bias[i]\n",
    "                        \n",
    "                        if val > alpha[t+1][(new_mask, i)]:\n",
    "                            alpha[t+1][(new_mask, i)] = val\n",
    "\n",
    "        # --- Backward (Beta) ---\n",
    "        beta = [defaultdict(lambda: -self.INF) for _ in range(N + 2)]\n",
    "        final_mask = (1 << N) - 1\n",
    "        \n",
    "        for i in range(N):\n",
    "            beta[N][(final_mask, i)] = S[i, depot]\n",
    "            \n",
    "        for t in range(N - 1, -1, -1):\n",
    "            for next_state, next_beta_val in beta[t+1].items():\n",
    "                if next_beta_val < -self.INF / 2: continue\n",
    "                next_mask, next_node = next_state\n",
    "                prev_mask = next_mask & ~(1 << next_node)\n",
    "                \n",
    "                xi_val = next_beta_val + bias[t][next_node]\n",
    "                \n",
    "                cands = [depot] if prev_mask == 0 else [j for j in range(N) if prev_mask & (1<<j)]\n",
    "                for prev in cands:\n",
    "                    val = S[prev, next_node] + xi_val\n",
    "                    if val > beta[t][(prev_mask, prev)]:\n",
    "                        beta[t][(prev_mask, prev)] = val\n",
    "\n",
    "        # --- Soft Output (Delta) ---\n",
    "        tilde_delta = np.zeros((N, N))\n",
    "        for t in range(N):\n",
    "            for i in range(N):\n",
    "                max_in = -self.INF\n",
    "                max_out = -self.INF\n",
    "                lam_i_for_i = -(1.0/N) * self.tilde_rho[t, i]\n",
    "                lam_i_for_j = ((N-1.0)/N) * self.tilde_rho[t, i]\n",
    "\n",
    "                for state, f_score in alpha[t+1].items():\n",
    "                    if f_score < -self.INF / 2: continue\n",
    "                    if state not in beta[t+1]: continue\n",
    "                    b_score = beta[t+1][state]\n",
    "                    if b_score < -self.INF / 2: continue\n",
    "                    \n",
    "                    total_score = f_score + b_score\n",
    "                    current_city = state[1]\n",
    "                    \n",
    "                    if current_city == i:\n",
    "                        zeta = total_score - lam_i_for_i\n",
    "                        if zeta > max_in: max_in = zeta\n",
    "                    else:\n",
    "                        zeta = total_score - lam_i_for_j\n",
    "                        if zeta > max_out: max_out = zeta\n",
    "                \n",
    "                if max_in < -self.INF / 2: diff = -self.INF\n",
    "                elif max_out < -self.INF / 2: diff = self.INF\n",
    "                else: diff = max_in - max_out\n",
    "                \n",
    "                tilde_delta[t, i] = diff\n",
    "                \n",
    "        return alpha, tilde_delta\n",
    "\n",
    "    def _run_bp(self, tilde_delta):\n",
    "        N = self.N\n",
    "        t_omega = self.tilde_phi + tilde_delta\n",
    "        new_eta = np.zeros_like(self.tilde_eta)\n",
    "        for i in range(N):\n",
    "            for t in range(N):\n",
    "                mask = np.ones(N, dtype=bool)\n",
    "                mask[t] = False\n",
    "                new_eta[t, i] = -np.max(t_omega[mask, i])\n",
    "        t_gamma = new_eta + tilde_delta\n",
    "        new_phi = np.zeros_like(self.tilde_phi)\n",
    "        for t in range(N):\n",
    "            for i in range(N):\n",
    "                mask = np.ones(N, dtype=bool)\n",
    "                mask[i] = False\n",
    "                new_phi[t, i] = -np.max(t_gamma[t, mask])\n",
    "        \n",
    "        self.tilde_eta = self.damping * self.tilde_eta + (1 - self.damping) * new_eta\n",
    "        self.tilde_phi = self.damping * self.tilde_phi + (1 - self.damping) * new_phi\n",
    "        self.tilde_rho = self.tilde_eta + self.tilde_phi\n",
    "\n",
    "    def _extract_path(self, alpha):\n",
    "        path = []\n",
    "        curr_mask = self.FULL_MASK\n",
    "        best_score = -self.INF\n",
    "        best_last = -1\n",
    "\n",
    "        # 1) 마지막 도시 선택\n",
    "        for i in range(self.N):\n",
    "            state = (curr_mask, i)\n",
    "            if state in alpha[self.N]:\n",
    "                score = alpha[self.N][state] + self.S[i, self.depot]\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_last = i\n",
    "\n",
    "        if best_last == -1:\n",
    "            return [], self.INF\n",
    "\n",
    "        # 초기 상태\n",
    "        path = [self.depot, best_last]\n",
    "        curr_node = best_last\n",
    "        curr_mask = self.FULL_MASK ^ (1 << best_last)\n",
    "\n",
    "        # 2) 역추적\n",
    "        for t in range(self.N - 1, 0, -1):\n",
    "\n",
    "            cands = [j for j in range(self.N) if curr_mask & (1 << j)]  # depot 제거\n",
    "\n",
    "            best_prev = -1\n",
    "            best_val = -self.INF\n",
    "\n",
    "            for prev in cands:\n",
    "                state = (curr_mask, prev)\n",
    "                if state in alpha[t]:\n",
    "                    val = alpha[t][state] + self.S[prev, curr_node]   # bias 제거 (이미 α에 포함됨)\n",
    "\n",
    "                    if val > best_val:\n",
    "                        best_val = val\n",
    "                        best_prev = prev\n",
    "\n",
    "            if best_prev == -1:\n",
    "                print(f\"Traceback failed at t={t}\")\n",
    "                return [], self.INF\n",
    "\n",
    "            path.append(best_prev)\n",
    "            curr_node = best_prev\n",
    "            curr_mask ^= (1 << best_prev)\n",
    "\n",
    "        path.append(self.depot)\n",
    "        path.reverse()\n",
    "\n",
    "        # cost 계산\n",
    "        cost = sum(self.dist_matrix[path[k], path[k+1]] for k in range(len(path)-1))\n",
    "        return path, cost\n",
    "\n",
    "    def solve(self):\n",
    "        best_global_path = []\n",
    "        best_global_cost = self.INF\n",
    "        \n",
    "        print(f\"Solving TSP (N={self.N}) with Alpha Visualization\")\n",
    "        \n",
    "        for it in range(self.bp_iterations):\n",
    "            alpha, tilde_delta = self._run_trellis()\n",
    "            path, cost = self._extract_path(alpha)\n",
    "            \n",
    "            if cost < best_global_cost:\n",
    "                best_global_cost = cost\n",
    "                best_global_path = path\n",
    "                print(f\"[Iter {it}] Cost: {cost:.2f} (New Best!) | Path: {path}\")\n",
    "            else:\n",
    "                if self.verbose:\n",
    "                     print(f\"[Iter {it}] Cost: {cost:.2f} | Path: {path}\")\n",
    "\n",
    "            self._run_bp(tilde_delta)\n",
    "            \n",
    "        return best_global_path, best_global_cost\n",
    "    \n",
    "    \n",
    "# --- 사용 예시 ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 임의의 거리 행렬 생성 (5x5)\n",
    "    # 실제 데이터는 파일에서 로드하거나 더 큰 N 사용\n",
    "    N_CITIES = 10\n",
    "    data = np.random.rand(N_CITIES, N_CITIES)\n",
    "    # TSP는 대칭이 아니어도 되지만, 보통 대각선 0\n",
    "    np.fill_diagonal(data, 0)\n",
    "    \n",
    "    solver = TSPSolverSOVA(data, bp_iterations=10, damping=0.8, verbose=True)\n",
    "    path, cost = solver.solve()\n",
    "    print(\"Final Path:\", path)\n",
    "    print(\"Final Cost:\", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe6472c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001] cost=1.343409498996 route=[0, 4, 7, 3, 8, 2, 1, 9, 6, 5, 0]\n",
      "[002] cost=1.343409498996 route=[0, 4, 7, 3, 8, 2, 1, 9, 6, 5, 0]\n",
      "[003] cost=1.343409498996 route=[0, 4, 7, 3, 8, 2, 1, 9, 6, 5, 0]\n",
      "[004] cost=1.343409498996 route=[0, 4, 7, 3, 8, 2, 1, 9, 6, 5, 0]\n",
      "[005] cost=1.343409498996 route=[0, 4, 7, 3, 8, 2, 1, 9, 6, 5, 0]\n",
      "[006] cost=1.343409498996 route=[0, 4, 7, 3, 8, 2, 1, 9, 6, 5, 0]\n",
      "[007] cost=1.343409498996 route=[0, 4, 7, 3, 8, 2, 1, 9, 6, 5, 0]\n",
      "[008] cost=1.343409498996 route=[0, 4, 7, 3, 8, 2, 1, 9, 6, 5, 0]\n",
      "[009] cost=1.343409498996 route=[0, 4, 7, 3, 8, 2, 1, 9, 6, 5, 0]\n",
      "[010] cost=1.343409498996 route=[0, 4, 7, 3, 8, 2, 1, 9, 6, 5, 0]\n",
      "[011] cost=1.343409498996 route=[0, 4, 7, 3, 8, 2, 1, 9, 6, 5, 0]\n",
      "Final Path: [0, 4, 7, 3, 8, 2, 1, 9, 6, 5, 0]\n",
      "Final Cost: 1.3434094989958751\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "NEG = -1e12  # numerical -inf\n",
    "\n",
    "\n",
    "class TSPHypercubeBCJR_SOVA:\n",
    "    \"\"\"\n",
    "    PDF 원래 식 기반으로 다시 정리한 BCJR + SOVA 버전.\n",
    "\n",
    "    - 단일 trellis ψ[t, mask, last]만 사용 (with-β)\n",
    "    - α_t(a)는 항상 ψ_{t-1}에서 직접 계산:\n",
    "        α_t(a) = max_{m_{t-1}, last ∈ m_{t-1}}\n",
    "                    [ ψ_{t-1}(m_{t-1}, last) + s(last, a) ]\n",
    "      (여기에는 β_t는 안 들어가고, 과거 시점 ≤ t-1의 β 정보만 포함)\n",
    "    - trellis → X_{it} 메시지:\n",
    "        ζ_{it}(a) = α_t(a) + β_t(a) - λ_{it}(a)\n",
    "      (∑_{i'≠i} λ_{i't}(a) = β_t(a) - λ_{it}(a) 사용한 형태)\n",
    "    - ψ는 with-β forward trellis:\n",
    "        ψ_t(m_t, a_t) = max_{m_{t-1}, a_{t-1}}\n",
    "            [ ψ_{t-1}(m_{t-1}, a_{t-1}) + s(a_{t-1}, a_t) + β_t(a_t) ]\n",
    "      (코드에서는 t=1..T, β_t(a_t)를 beta[t-1, a_t]에 매핑)\n",
    "    - BCJR backward bwd[t, mask, last]와 함께\n",
    "      Γ_t(m_t,last) = ψ_t(m_t,last) + bwd_t(m_t,last)로 SOVA LLR 계산.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, D, start_city=None,\n",
    "                 damping=0.3, iters=200, verbose=False,\n",
    "                 tiny_tiebreak=False, seed=0,\n",
    "                 patience_no_cost_change=10, cost_tol=1e-12,\n",
    "                 kappa_bcjr=1.0,\n",
    "                 damp_L=0.5, damp_beta=0.5, damp_zeta=0.5):\n",
    "\n",
    "        D = np.array(D, dtype=float)\n",
    "        assert D.shape[0] == D.shape[1], \"D must be square\"\n",
    "        C = D.shape[0]\n",
    "\n",
    "        if start_city is None:\n",
    "            start_city = 0\n",
    "        start_city = int(start_city)\n",
    "        assert 0 <= start_city < C\n",
    "\n",
    "        # permute: start -> last (internal depot)\n",
    "        perm = np.arange(C)\n",
    "        if start_city != C - 1:\n",
    "            perm[start_city], perm[C - 1] = perm[C - 1], perm[start_city]\n",
    "        inv_perm = np.empty(C, dtype=int)\n",
    "        inv_perm[perm] = np.arange(C)\n",
    "\n",
    "        self.orig_D = D\n",
    "        self.D = D[perm][:, perm]\n",
    "        self.perm = perm\n",
    "        self.inv_perm = inv_perm\n",
    "\n",
    "        self.C = C\n",
    "        self.N = C - 1\n",
    "        self.depot = C - 1\n",
    "\n",
    "        self.verbose = verbose\n",
    "        self.damp = float(damping)\n",
    "        self.iters = int(iters)\n",
    "        self.tiny_tiebreak = bool(tiny_tiebreak)\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.patience_no_cost_change = int(patience_no_cost_change)\n",
    "        self.cost_tol = float(cost_tol)\n",
    "\n",
    "        # BCJR / SOVA 관련 파라미터\n",
    "        self.kappa_bcjr = float(kappa_bcjr)\n",
    "        self.damp_L = float(damp_L)\n",
    "        self.damp_beta = float(damp_beta)\n",
    "        self.damp_zeta = float(damp_zeta)\n",
    "\n",
    "        # similarity (bigger is better)\n",
    "        mx = np.max(self.D)\n",
    "        self.s = mx - self.D\n",
    "\n",
    "        # trellis 크기\n",
    "        self.T = self.N\n",
    "        self.M = 1 << self.N\n",
    "\n",
    "        # 단일 forward trellis ψ (with-β)\n",
    "        self.psi = np.full((self.T + 1, self.M, self.N), NEG)\n",
    "        self.backptr = np.full((self.T + 1, self.M, self.N, 2), -1, dtype=int)\n",
    "\n",
    "        # backward trellis (with-β, closure 포함)\n",
    "        self.bwd_wb = np.full((self.T + 1, self.M, self.N), NEG)\n",
    "\n",
    "        # α_t(a): ψ_{t-1}로부터 계산\n",
    "        self.alpha = np.full((self.T, self.N), NEG)\n",
    "\n",
    "        # simplified messages\n",
    "        self.gamma_tilde = np.zeros((self.N, self.T))\n",
    "        self.omega_tilde = np.zeros((self.N, self.T))\n",
    "        self.phi_tilde   = np.zeros((self.N, self.T))\n",
    "        self.eta_tilde   = np.zeros((self.N, self.T))\n",
    "        self.rho_tilde   = np.zeros((self.N, self.T))\n",
    "        self.delta_tilde = np.zeros((self.N, self.T))\n",
    "\n",
    "        # λ_t(i,a), ζ_t(i,a), β_t(a)\n",
    "        self.lambda_ = np.zeros((self.T, self.N, self.N))\n",
    "        self.zeta    = np.zeros((self.T, self.N, self.N))\n",
    "        self.beta    = np.zeros((self.T, self.N))\n",
    "\n",
    "        # damping 캐시\n",
    "        self._L_prev    = [np.zeros((self.N, self.N)) for _ in range(self.T)]\n",
    "        self._beta_prev = [np.zeros(self.N)            for _ in range(self.T)]\n",
    "        self._zeta_prev = [np.zeros((self.N, self.N)) for _ in range(self.T)]\n",
    "\n",
    "    # ===================== Public =====================\n",
    "    def run(self):\n",
    "        stable = 0\n",
    "        last_cost = None\n",
    "        best_route, best_cost = None, None\n",
    "\n",
    "        for it in range(self.iters):\n",
    "            # (1) forward trellis & α\n",
    "            self._trellis_forward_and_alpha()\n",
    "\n",
    "            # (2) backward trellis (primal with β + closure)\n",
    "            self._trellis_backward_bcjr()\n",
    "\n",
    "            # (3) BCJR-style SOVA LLR 계산 (T x N)\n",
    "            llr_t = self._compute_sova_llr_from_bcjr()\n",
    "\n",
    "            # (4) 메시지 업데이트\n",
    "            self._update_phi_eta_rho()\n",
    "            self._update_lambda_beta_zeta_delta(\n",
    "                kappa=self.kappa_bcjr,\n",
    "                llr_t=llr_t,\n",
    "                damp_L=self.damp_L,\n",
    "                damp_beta=self.damp_beta,\n",
    "                damp_zeta=self.damp_zeta,\n",
    "            )\n",
    "            self._update_gamma_omega()\n",
    "\n",
    "            if self.tiny_tiebreak:\n",
    "                self.gamma_tilde += 1e-12 * self.rng.standard_normal(self.gamma_tilde.shape)\n",
    "\n",
    "            # (5) route & cost\n",
    "            route = self.estimate_route()\n",
    "            cost = self._route_cost(route)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"[{it+1:03d}] cost={cost:.12f} route={route}\")\n",
    "\n",
    "            # best primal 추적\n",
    "            if best_cost is None or cost < best_cost:\n",
    "                best_cost, best_route = cost, route\n",
    "\n",
    "            # plateau early stop\n",
    "            if last_cost is not None and abs(cost - last_cost) <= self.cost_tol:\n",
    "                stable += 1\n",
    "            else:\n",
    "                stable = 0\n",
    "            last_cost = cost\n",
    "\n",
    "            if stable >= self.patience_no_cost_change:\n",
    "                return best_route, best_cost\n",
    "\n",
    "        return best_route, best_cost\n",
    "\n",
    "    # ===================== Trellis (single ψ) & α =====================\n",
    "    def _trellis_forward_and_alpha(self):\n",
    "        \"\"\"\n",
    "        ψ_t(m_t, last) : with-β forward metric\n",
    "        α_t(a)         : ψ_{t-1}로부터 계산 (현재 시점 β_t는 포함되지 않음)\n",
    "        \"\"\"\n",
    "        self.psi.fill(NEG)\n",
    "        self.backptr.fill(-1)\n",
    "        self.alpha.fill(NEG)\n",
    "\n",
    "        # t = 1\n",
    "        # α_1(a) = s(depot, a)\n",
    "        # ψ_1(m={a}, a) = s(depot, a) + β_1(a)\n",
    "        for a in range(self.N):\n",
    "            m = 1 << a\n",
    "            gain = self.s[self.depot, a]\n",
    "            self.alpha[0, a] = gain                    # α_1(a)\n",
    "            self.psi[1, m, a] = gain + self.beta[0, a]  # ψ_1\n",
    "            self.backptr[1, m, a] = (0, -1)             # 센티넬\n",
    "\n",
    "        # t = 2..T\n",
    "        full_mask = (1 << self.N) - 1\n",
    "        for t in range(2, self.T + 1):\n",
    "            # 1) α_t(a) 계산: ψ_{t-1}에서 오는 factor→A_t 메시지\n",
    "            #    α_t(a) = max_{m_{t-1},last∈m_{t-1}, a∉m_{t-1}}\n",
    "            #                 [ ψ_{t-1}(m_{t-1},last) + s(last,a) ]\n",
    "            for a in range(self.N):\n",
    "                best_alpha = NEG\n",
    "                for mask in range(self.M):\n",
    "                    if mask == 0 or mask.bit_count() != (t - 1):\n",
    "                        continue\n",
    "                    if mask & (1 << a):\n",
    "                        continue  # 아직 방문 안 한 도시만\n",
    "                    m = mask\n",
    "                    while m:\n",
    "                        last = (m & -m).bit_length() - 1\n",
    "                        m ^= (1 << last)\n",
    "                        cand = self.psi[t - 1, mask, last] + self.s[last, a]\n",
    "                        if cand > best_alpha:\n",
    "                            best_alpha = cand\n",
    "                self.alpha[t - 1, a] = best_alpha\n",
    "\n",
    "            # 2) ψ_t 전이 (with-β)\n",
    "            for mask in range(self.M):\n",
    "                if mask == 0 or mask.bit_count() != (t - 1):\n",
    "                    continue\n",
    "                for a in range(self.N):\n",
    "                    if mask & (1 << a):\n",
    "                        continue\n",
    "                    new_mask = mask | (1 << a)\n",
    "                    best = NEG\n",
    "                    best_last = -1\n",
    "\n",
    "                    m = mask\n",
    "                    while m:\n",
    "                        last = (m & -m).bit_length() - 1\n",
    "                        m ^= (1 << last)\n",
    "                        if last == a:\n",
    "                            continue  # self-loop 금지\n",
    "\n",
    "                        # ψ_{t-1} + s(last,a) + β_t(a)\n",
    "                        cand = self.psi[t - 1, mask, last] + self.s[last, a] + self.beta[t - 1, a]\n",
    "                        if cand > best:\n",
    "                            best = cand\n",
    "                            best_last = last\n",
    "\n",
    "                    if best > self.psi[t, new_mask, a]:\n",
    "                        self.psi[t, new_mask, a] = best\n",
    "                        self.backptr[t, new_mask, a] = (mask, best_last)\n",
    "\n",
    "    # ===================== Backward trellis (BCJR용) =====================\n",
    "    def _trellis_backward_bcjr(self):\n",
    "        \"\"\"\n",
    "        bwd_wb[t, mask, last]:\n",
    "          - t 시점에 (mask,last) 상태에서 시작해서\n",
    "          - 나머지 도시 방문 + depot으로 귀환까지의 최대 future metric.\n",
    "        전이:\n",
    "          - t < T:\n",
    "              bwd[t,mask,last] = max_{a not in mask} { s[last,a] + β_{t+1}(a) + bwd[t+1, mask|{a}, a] }\n",
    "            (코드에선 β_{t+1}(a)를 beta[t, a]로 저장)\n",
    "          - t = T:\n",
    "              full_mask 에 대해서만 closure: s[last, depot]\n",
    "        \"\"\"\n",
    "        self.bwd_wb.fill(NEG)\n",
    "        full_mask = (1 << self.N) - 1\n",
    "\n",
    "        # t = T: full mask에서 depot으로 가는 closure\n",
    "        t = self.T\n",
    "        for last in range(self.N):\n",
    "            mask = full_mask\n",
    "            self.bwd_wb[t, mask, last] = self.s[last, self.depot]\n",
    "\n",
    "        # t = T-1..1 역순\n",
    "        for t in range(self.T - 1, 0, -1):\n",
    "            for mask in range(self.M):\n",
    "                if mask == 0 or mask.bit_count() != t:\n",
    "                    continue\n",
    "                for last in range(self.N):\n",
    "                    if not (mask & (1 << last)):\n",
    "                        continue\n",
    "\n",
    "                    best = NEG\n",
    "                    avail = (~mask) & full_mask\n",
    "                    m = avail\n",
    "                    while m:\n",
    "                        a = (m & -m).bit_length() - 1\n",
    "                        m ^= (1 << a)\n",
    "                        new_mask = mask | (1 << a)\n",
    "                        cand = self.s[last, a] + self.beta[t, a] + self.bwd_wb[t + 1, new_mask, a]\n",
    "                        if cand > best:\n",
    "                            best = cand\n",
    "                    self.bwd_wb[t, mask, last] = best\n",
    "\n",
    "    # ===================== SOVA-style LLR from BCJR =====================\n",
    "    def _compute_sova_llr_from_bcjr(self):\n",
    "        \"\"\"\n",
    "        llr[t, i] ≈\n",
    "          max_{mask, last=i, |mask|=t+1} [ ψ[t+1, mask, i] + bwd[t+1, mask, i] ]\n",
    "          - max_{mask, last≠i, |mask|=t+1} [ ψ[t+1, mask, last] + bwd[t+1, mask, last] ]\n",
    "        여기서 내부 시간 인덱스(t+1)는 1..T 와 매칭, 외부 t는 0..T-1.\n",
    "        \"\"\"\n",
    "        T, N, M = self.T, self.N, self.M\n",
    "        llr = np.zeros((T, N), dtype=float)\n",
    "\n",
    "        for t in range(1, T + 1):  # 내부 시간: 1..T\n",
    "            for i in range(N):\n",
    "                best_with = NEG\n",
    "                best_without = NEG\n",
    "\n",
    "                for mask in range(M):\n",
    "                    if mask == 0 or mask.bit_count() != t:\n",
    "                        continue\n",
    "\n",
    "                    # last = i 인 state\n",
    "                    val_i = self.psi[t, mask, i] + self.bwd_wb[t, mask, i]\n",
    "                    if val_i > best_with:\n",
    "                        best_with = val_i\n",
    "\n",
    "                    # last ≠ i 인 state들 중 최고값\n",
    "                    m2 = mask\n",
    "                    while m2:\n",
    "                        last = (m2 & -m2).bit_length() - 1\n",
    "                        m2 ^= (1 << last)\n",
    "                        if last == i:\n",
    "                            continue\n",
    "                        val = self.psi[t, mask, last] + self.bwd_wb[t, mask, last]\n",
    "                        if val > best_without:\n",
    "                            best_without = val\n",
    "\n",
    "                if best_with <= NEG / 2 and best_without <= NEG / 2:\n",
    "                    llr[t - 1, i] = 0.0\n",
    "                else:\n",
    "                    llr[t - 1, i] = best_with - best_without\n",
    "\n",
    "        return llr\n",
    "\n",
    "    # ===================== Messages =====================\n",
    "    def _update_phi_eta_rho(self):\n",
    "        # φ̃_it = -max_{i'≠i} γ̃_i't\n",
    "        for t in range(self.T):\n",
    "            col = self.gamma_tilde[:, t]\n",
    "            for i in range(self.N):\n",
    "                self.phi_tilde[i, t] = -np.max(np.delete(col, i)) if self.N > 1 else 0.0\n",
    "        # η̃_it = -max_{t'≠t} ω̃_it'\n",
    "        for i in range(self.N):\n",
    "            row = self.omega_tilde[i, :]\n",
    "            for t in range(self.T):\n",
    "                self.eta_tilde[i, t] = -np.max(np.delete(row, t)) if self.T > 1 else 0.0\n",
    "        # ρ̃_it\n",
    "        self.rho_tilde = self.eta_tilde + self.phi_tilde\n",
    "\n",
    "    def _update_lambda_beta_zeta_delta(self, kappa=0.0, llr_t=None,\n",
    "                                       damp_L=0.5, damp_beta=0.5, damp_zeta=0.5):\n",
    "        T, N = self.T, self.N\n",
    "\n",
    "        for t in range(T):\n",
    "            # (1) rhõ -> (rho0, rho1) 복원\n",
    "            r = self.rho_tilde[:, t]  # shape (N,)\n",
    "            rho0 = -r / N             # off-diagonal\n",
    "            rho1 = rho0 + r           # diagonal\n",
    "\n",
    "            # (2) L_new(i,a): a==i→rho1, else→rho0\n",
    "            L_new = np.empty((N, N), float)\n",
    "            for i in range(N):\n",
    "                L_new[i, :] = rho0[i]\n",
    "                L_new[i, i] = rho1[i]\n",
    "\n",
    "            # (3) λ 이중 센터링\n",
    "            L_new -= L_new.mean(axis=1, keepdims=True)\n",
    "            L_new -= L_new.mean(axis=0, keepdims=True)\n",
    "\n",
    "            # (4) λ damping\n",
    "            L_prev = self._L_prev[t]\n",
    "            L = damp_L * L_new + (1 - damp_L) * L_prev\n",
    "            self.lambda_[t] = L\n",
    "\n",
    "            # (5) β_t(a) = sum_i λ_{it}(a)\n",
    "            beta_new = L.sum(axis=0)\n",
    "            beta_new -= beta_new.mean()\n",
    "\n",
    "            # (6) β damping\n",
    "            beta_prev = self._beta_prev[t]\n",
    "            beta_t = damp_beta * beta_new + (1 - damp_beta) * beta_prev\n",
    "            self.beta[t, :] = beta_t\n",
    "\n",
    "            # (7) ζ_it(a) = α_t(a) + β_t(a) - λ_it(a)\n",
    "            a_t = self.alpha[t, :]\n",
    "            Z_new = a_t[np.newaxis, :] + beta_t[np.newaxis, :] - L\n",
    "\n",
    "            # (8) SOVA LLR 주입 (대각 성분)\n",
    "            if kappa and llr_t is not None:\n",
    "                for i in range(N):\n",
    "                    Z_new[i, i] += kappa * llr_t[t, i]\n",
    "\n",
    "            # (9) ζ damping\n",
    "            Z_prev = self._zeta_prev[t]\n",
    "            Z = damp_zeta * Z_new + (1 - damp_zeta) * Z_prev\n",
    "            self.zeta[t] = Z\n",
    "\n",
    "            # (10) δ̃_it = ζ_it(i) - max_{a≠i} ζ_it(a)\n",
    "            for i in range(N):\n",
    "                zi = Z[i, :]\n",
    "                self.delta_tilde[i, t] = 0.0 if N == 1 else (zi[i] - np.max(np.delete(zi, i)))\n",
    "\n",
    "        # 캐시 갱신\n",
    "        self._L_prev    = [self.lambda_[t].copy() for t in range(T)]\n",
    "        self._beta_prev = [self.beta[t].copy()    for t in range(T)]\n",
    "        self._zeta_prev = [self.zeta[t].copy()    for t in range(T)]\n",
    "\n",
    "    def _update_gamma_omega(self):\n",
    "        gamma_new = self.eta_tilde + self.delta_tilde\n",
    "        omega_new = self.phi_tilde + self.delta_tilde\n",
    "        self.gamma_tilde = self.damp * gamma_new + (1 - self.damp) * self.gamma_tilde\n",
    "        self.omega_tilde = self.damp * omega_new + (1 - self.damp) * self.omega_tilde\n",
    "\n",
    "    # ===================== Decode =====================\n",
    "    def estimate_route(self):\n",
    "        full_mask = (1 << self.N) - 1\n",
    "        best_val = NEG\n",
    "        best_last = -1\n",
    "\n",
    "        for last in range(self.N):\n",
    "            base = self.psi[self.T, full_mask, last]\n",
    "            if base <= NEG / 2:\n",
    "                continue\n",
    "            val = base + self.s[last, self.depot]  # closure\n",
    "            if val > best_val:\n",
    "                best_val = val\n",
    "                best_last = last\n",
    "\n",
    "        # backtrack\n",
    "        if best_last < 0:\n",
    "            # fallback: α 기반 greedy\n",
    "            route_internal = [self.depot]\n",
    "            used = set()\n",
    "            for t in range(self.T):\n",
    "                sc = self.alpha[t].copy()\n",
    "                for u in used:\n",
    "                    sc[u] = NEG\n",
    "                if self.tiny_tiebreak:\n",
    "                    sc += 1e-15 * np.arange(self.N)\n",
    "                a = int(np.argmax(sc))\n",
    "                used.add(a)\n",
    "                route_internal.append(a)\n",
    "            route_internal.append(self.depot)\n",
    "        else:\n",
    "            route_inner = []\n",
    "            mask = full_mask\n",
    "            last = best_last\n",
    "            t = self.T\n",
    "            while t > 0 and 0 <= last < self.N:\n",
    "                route_inner.append(last)\n",
    "                prev_mask, prev_last = self.backptr[t, mask, last]\n",
    "                mask, last = prev_mask, prev_last\n",
    "                t -= 1\n",
    "            route_inner.reverse()\n",
    "            route_internal = [self.depot] + route_inner + [self.depot]\n",
    "\n",
    "        return [int(self.inv_perm[c]) for c in route_internal]\n",
    "\n",
    "    def _route_cost(self, route):\n",
    "        return float(sum(self.orig_D[route[k], route[k + 1]] for k in range(len(route) - 1)))\n",
    "\n",
    "\n",
    "    \n",
    "# --- 사용 예시 ---     \n",
    "if __name__ == \"__main__\":\n",
    "    # 임의의 거리 행렬 생성 (5x5)\n",
    "    # 실제 데이터는 파일에서 로드하거나 더 큰 N 사용\n",
    "    N_CITIES = 10\n",
    "    data = np.random.rand(N_CITIES, N_CITIES)\n",
    "    # TSP는 대칭이 아니어도 되지만, 보통 대각선 0\n",
    "    np.fill_diagonal(data, 0)\n",
    "    \n",
    "    solver = TSPHypercubeBCJR_SOVA(data, verbose=True)\n",
    "    path, cost = solver.run()\n",
    "    print(\"Final Path:\", path)\n",
    "    print(\"Final Cost:\", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f41d1bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
